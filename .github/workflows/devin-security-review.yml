# Devin Security Review — Closed Feedback Loop
#
# This workflow runs on every PR to main. It waits for CodeQL to finish,
# fetches alerts, classifies them (new-in-PR vs pre-existing), and creates
# Devin sessions to fix them.
#
# KEY DESIGN DECISIONS (see DESIGN.md for full rationale):
#
# 1. COMMENT MANAGEMENT: We find-or-update a single PR comment using a hidden
#    HTML marker (<!-- devin-security-review -->). This prevents comment flooding
#    when Devin pushes trigger re-runs of this workflow.
#
# 2. ATTEMPT TRACKING: Hidden markers in the comment body track how many times
#    each alert has been attempted. After 2 failed attempts, the alert is marked
#    "unfixable" and skipped on future runs. This breaks the infinite loop.
#
# 3. DEVIN-COMMIT DETECTION: If the latest commit message matches the pattern
#    "fix: [rule_id]...", we know this run was triggered by a Devin fix push.
#    We apply stricter filtering (only process genuinely new alerts).
#
# 4. LOCAL CODEQL VERIFICATION: The Devin prompt instructs Devin to install the
#    CodeQL CLI and run analysis locally (same config: security-and-quality suite,
#    remote+local threat models) before pushing. If the alert persists after 2
#    local attempts, Devin skips it and reports it as unfixable.
#
# 5. RATE LIMIT HANDLING: Before creating each Devin session, we check the
#    Devin API response for errors and implement exponential backoff on failures.
#    GitHub API rate limits are checked before bulk operations.
#
# 6. SAFE-TO-MERGE SIGNAL: The PR comment includes a clear status section:
#    - "All alerts resolved" if everything is fixed
#    - "REQUIRES MANUAL REVIEW" with details if any alerts are unfixable
#    A GitHub label (devin:manual-review-needed) is added when manual review is needed.
#
# 7. RESOURCE CAPS: max_acu_limit on Devin sessions, idempotent flag to prevent
#    duplicate sessions, max 20 sessions per workflow run, 3 concurrent.

name: Devin Security Review

on:
  workflow_run:
    workflows: ["CodeQL"]
    types: [completed]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      debug:
        description: "Enable verbose debug output in PR comment"
        required: false
        type: boolean
        default: false
      pr_number:
        description: "PR number to analyze (for manual runs)"
        required: false
        type: string

permissions:
  contents: read
  security-events: write
  pull-requests: write

env:
  DEBUG: ${{ github.event.inputs.debug == 'false' && 'false' || 'true' }}

jobs:
  security-review:
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'pull_request' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success' && github.event.workflow_run.event == 'pull_request')
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      # ------------------------------------------------------------------
      # STEP 0a: Resolve PR context.
      # workflow_run doesn't have github.event.pull_request, so we resolve
      # the PR number and head SHA from the triggering workflow's context.
      # This step normalizes the context for all three trigger types.
      # ------------------------------------------------------------------
      - name: Resolve PR context
        id: pr-context
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          WR_HEAD_BRANCH: ${{ github.event.workflow_run.head_branch }}
          WR_PULL_REQUESTS: ${{ toJson(github.event.workflow_run.pull_requests) }}
        run: |
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            echo "Triggered by workflow_run (CodeQL completed)"
            HEAD_BRANCH="$WR_HEAD_BRANCH"
            HEAD_SHA="${{ github.event.workflow_run.head_sha }}"

            PR_NUMBER=$(echo "$WR_PULL_REQUESTS" | jq -r '.[0].number // empty')

            if [ -z "$PR_NUMBER" ]; then
              echo "pull_requests array empty — searching by branch..."
              PR_NUMBER=$(curl -s -L \
                -H "Authorization: token $GH_PAT" \
                -H "Accept: application/vnd.github+json" \
                "https://api.github.com/repos/${{ github.repository }}/pulls?state=open&head=${{ github.repository_owner }}:${HEAD_BRANCH}" \
                | jq -r '.[0].number // empty')
            fi

            if [ -z "$PR_NUMBER" ]; then
              echo "::warning::Could not resolve PR number from workflow_run. Skipping."
              echo "skip=true" >> $GITHUB_OUTPUT
              exit 0
            fi

            echo "Resolved: PR #$PR_NUMBER, branch=$HEAD_BRANCH, sha=$HEAD_SHA"
            echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
            echo "head_sha=$HEAD_SHA" >> $GITHUB_OUTPUT
            echo "head_branch=$HEAD_BRANCH" >> $GITHUB_OUTPUT
            echo "trigger=workflow_run" >> $GITHUB_OUTPUT
            echo "skip=false" >> $GITHUB_OUTPUT

          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "Triggered by pull_request"
            echo "pr_number=${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
            echo "head_sha=${{ github.event.pull_request.head.sha }}" >> $GITHUB_OUTPUT
            echo "head_branch=${{ github.head_ref }}" >> $GITHUB_OUTPUT
            echo "trigger=pull_request" >> $GITHUB_OUTPUT
            echo "skip=false" >> $GITHUB_OUTPUT

          elif [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "Triggered by workflow_dispatch"
            PR_NUM="${{ github.event.inputs.pr_number }}"
            if [ -z "$PR_NUM" ]; then
              echo "::error::PR number required for manual dispatch."
              echo "skip=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            PR_DATA=$(curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/${{ github.repository }}/pulls/$PR_NUM")
            HEAD_SHA=$(echo "$PR_DATA" | jq -r '.head.sha')
            HEAD_BRANCH=$(echo "$PR_DATA" | jq -r '.head.ref')
            echo "pr_number=$PR_NUM" >> $GITHUB_OUTPUT
            echo "head_sha=$HEAD_SHA" >> $GITHUB_OUTPUT
            echo "head_branch=$HEAD_BRANCH" >> $GITHUB_OUTPUT
            echo "trigger=workflow_dispatch" >> $GITHUB_OUTPUT
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      # ------------------------------------------------------------------
      # STEP 0: Health check — validate secrets and API availability.
      # If Devin API is unreachable, we enter "graceful degradation" mode:
      # the workflow still posts alert summaries on the PR, but skips
      # session creation. CodeQL's required status check still blocks
      # merging of vulnerable code — our workflow is advisory, not a gate.
      # ------------------------------------------------------------------
      - name: Health check (validate secrets and Devin API)
        id: health-check
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
        run: |
          HEALTHY=true
          DEVIN_AVAILABLE=true

          # Check GH_PAT
          if [ -z "$GH_PAT" ]; then
            echo "::error::GH_PAT secret is not set. The workflow cannot access the GitHub API."
            HEALTHY=false
          else
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
              -H "Authorization: token $GH_PAT" \
              "https://api.github.com/repos/${{ github.repository }}")
            if [ "$HTTP_CODE" != "200" ]; then
              echo "::error::GH_PAT returned HTTP $HTTP_CODE — token may be expired or lack required scopes (repo, security_events)."
              HEALTHY=false
            else
              echo "GH_PAT: valid (HTTP 200)"
            fi
          fi

          # Check DEVIN_API_KEY
          if [ -z "$DEVIN_API_KEY" ]; then
            echo "::warning::DEVIN_API_KEY secret is not set. Entering degraded mode — alerts will be reported but not auto-fixed."
            DEVIN_AVAILABLE=false
          else
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
              -X GET "https://api.devin.ai/v1/sessions" \
              -H "Authorization: Bearer $DEVIN_API_KEY")
            if [ "$HTTP_CODE" = "401" ] || [ "$HTTP_CODE" = "403" ]; then
              echo "::warning::DEVIN_API_KEY returned HTTP $HTTP_CODE — key may be invalid. Entering degraded mode."
              DEVIN_AVAILABLE=false
            elif [ "$HTTP_CODE" = "000" ]; then
              echo "::warning::Devin API is unreachable (connection failed). Entering degraded mode."
              DEVIN_AVAILABLE=false
            else
              echo "Devin API: reachable (HTTP $HTTP_CODE)"
            fi
          fi

          if [ "$HEALTHY" = "false" ]; then
            echo "::error::Health check failed — critical secrets are missing or invalid."
            exit 1
          fi

          echo "devin_available=$DEVIN_AVAILABLE" >> $GITHUB_OUTPUT
          if [ "$DEVIN_AVAILABLE" = "false" ]; then
            echo "::warning::Running in DEGRADED MODE — Devin API unavailable. Alerts will be reported but auto-fix is disabled. CodeQL still blocks merging of vulnerable code."
          else
            echo "Health check passed — all systems operational."
          fi

          # Save debug data for PR comment
          echo "{\"gh_pat_valid\": $([ \"$HEALTHY\" = \"true\" ] && echo true || echo false), \"devin_available\": $([ \"$DEVIN_AVAILABLE\" = \"true\" ] && echo true || echo false)}" > /tmp/debug_health.json

      # ------------------------------------------------------------------
      # STEP 1: Detect if this run was triggered by a Devin fix commit.
      # If so, we set a flag to apply stricter alert filtering downstream.
      # This prevents the infinite loop: Devin push -> workflow run ->
      # same alert found -> new session -> Devin push -> ...
      # ------------------------------------------------------------------
      - name: Detect Devin fix commit
        id: detect-devin
        run: |
          LATEST_MSG=$(git log -1 --pretty=%s --no-merges HEAD)
          echo "Latest commit message (skipping merges): $LATEST_MSG"

          if echo "$LATEST_MSG" | grep -qE '^fix: \['; then
            echo "is_devin_fix=true" >> $GITHUB_OUTPUT
            echo "Detected Devin fix commit — will apply strict alert filtering"
          else
            echo "is_devin_fix=false" >> $GITHUB_OUTPUT
            echo "Normal commit — standard processing"
          fi

      # ------------------------------------------------------------------
      # STEP 2: Wait for all CodeQL analyses to complete.
      # Polls GitHub check runs API every 30s until all CodeQL-related
      # checks report "completed". Timeout after 20 minutes (40 * 30s).
      # ------------------------------------------------------------------
      - name: Wait for all CodeQL analyses to complete
        if: steps.pr-context.outputs.trigger != 'workflow_run'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          echo "Waiting for all CodeQL analyses to complete..."
          REPO="${{ github.repository }}"
          SHA="${{ steps.pr-context.outputs.head_sha }}"
          MAX_ATTEMPTS=40
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS - Checking CodeQL check runs..."

            CHECK_RUNS=$(curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/commits/$SHA/check-runs")

            CODEQL_RUNS=$(echo "$CHECK_RUNS" | jq '[.check_runs[] | select(.name == "CodeQL" or (.name | startswith("Analyze")))]')
            TOTAL=$(echo "$CODEQL_RUNS" | jq 'length')
            COMPLETED=$(echo "$CODEQL_RUNS" | jq '[.[] | select(.status == "completed")] | length')

            echo "  CodeQL-related check runs: $COMPLETED/$TOTAL completed"
            echo "$CODEQL_RUNS" | jq -r '.[] | "    - \(.name): status=\(.status), conclusion=\(.conclusion // "pending")"'

            if [ "$TOTAL" -gt 0 ] && [ "$COMPLETED" -eq "$TOTAL" ]; then
              echo "All CodeQL analyses completed."
              break
            fi

            if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
              echo "Timed out waiting for CodeQL. Proceeding with available results..."
              break
            fi

            echo "  Sleeping 30s..."
            sleep 30
          done

      # ------------------------------------------------------------------
      # STEP 3: Fetch and classify CodeQL alerts.
      # Fetches all open alerts on the PR merge ref, then compares with
      # alerts on main to classify as "new in PR" vs "pre-existing".
      # Alerts are sorted by severity (critical > high > medium > low).
      # ------------------------------------------------------------------
      - name: Fetch and classify CodeQL alerts
        id: classify-alerts
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          PR_NUMBER: ${{ steps.pr-context.outputs.pr_number }}
        run: |
          REPO="${{ github.repository }}"
          PR_NUMBER="$PR_NUMBER"
          PR_REF="refs/pull/${PR_NUMBER}/merge"

          echo "Fetching code scanning alerts for $REPO ref=$PR_REF ..."

          # Fetch ALL alerts with pagination (enterprise repos can have >100 alerts)
          PAGE=1
          echo '[]' > /tmp/pr_alerts_paginated.json
          while true; do
            RESP=$(curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/code-scanning/alerts?ref=$PR_REF&state=open&per_page=100&page=$PAGE")
            PAGE_COUNT=$(echo "$RESP" | jq 'if type == "array" then length else 0 end')
            if [ "$PAGE_COUNT" -eq 0 ] || [ "$PAGE_COUNT" = "null" ]; then
              break
            fi
            echo "$RESP" > /tmp/pr_alerts_page.json
            jq -s '.[0] + .[1]' /tmp/pr_alerts_paginated.json /tmp/pr_alerts_page.json > /tmp/pr_alerts_merged.json
            mv /tmp/pr_alerts_merged.json /tmp/pr_alerts_paginated.json
            if [ "$PAGE_COUNT" -lt 100 ]; then
              break
            fi
            PAGE=$((PAGE + 1))
          done
          PR_ALERTS=$(cat /tmp/pr_alerts_paginated.json)

          PR_ALERT_COUNT=$(echo "$PR_ALERTS" | jq 'if type == "array" then length else 0 end')
          echo "Found $PR_ALERT_COUNT alert(s) on PR merge ref (across $PAGE page(s))"

          if [ "$PR_ALERT_COUNT" -eq 0 ] || [ "$PR_ALERT_COUNT" = "null" ]; then
            echo "No alerts found."
            echo "total_count=0" >> $GITHUB_OUTPUT
            echo "new_in_pr_count=0" >> $GITHUB_OUTPUT
            echo "preexisting_count=0" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Fetching alerts on main branch for comparison..."
          PAGE=1
          echo '[]' > /tmp/main_alerts_paginated.json
          while true; do
            RESP=$(curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/code-scanning/alerts?ref=refs/heads/main&state=open&per_page=100&page=$PAGE")
            PAGE_COUNT=$(echo "$RESP" | jq 'if type == "array" then length else 0 end')
            if [ "$PAGE_COUNT" -eq 0 ] || [ "$PAGE_COUNT" = "null" ]; then
              break
            fi
            echo "$RESP" > /tmp/main_alerts_page.json
            jq -s '.[0] + .[1]' /tmp/main_alerts_paginated.json /tmp/main_alerts_page.json > /tmp/main_alerts_merged.json
            mv /tmp/main_alerts_merged.json /tmp/main_alerts_paginated.json
            if [ "$PAGE_COUNT" -lt 100 ]; then
              break
            fi
            PAGE=$((PAGE + 1))
          done
          MAIN_ALERTS=$(cat /tmp/main_alerts_paginated.json)

          MAIN_ALERT_COUNT=$(echo "$MAIN_ALERTS" | jq 'if type == "array" then length else 0 end')
          echo "Found $MAIN_ALERT_COUNT alert(s) on main (across $PAGE page(s))"

          echo "$PR_ALERTS" > /tmp/pr_alerts_raw.json
          echo "$MAIN_ALERTS" > /tmp/main_alerts_raw.json

          python3 << 'CLASSIFY_EOF'
          import json, os

          with open("/tmp/pr_alerts_raw.json") as f:
              pr_alerts_raw = f.read()
          with open("/tmp/main_alerts_raw.json") as f:
              main_alerts_raw = f.read()

          try:
              pr_alerts = json.loads(pr_alerts_raw)
              if not isinstance(pr_alerts, list):
                  pr_alerts = []
          except Exception:
              pr_alerts = []

          try:
              main_alerts = json.loads(main_alerts_raw)
              if not isinstance(main_alerts, list):
                  main_alerts = []
          except Exception:
              main_alerts = []

          main_rule_file_set = set()
          for a in main_alerts:
              rule_id = a.get("rule", {}).get("id", "")
              loc = a.get("most_recent_instance", {}).get("location", {})
              path = loc.get("path", "")
              line = loc.get("start_line", 0)
              main_rule_file_set.add(f"{rule_id}:{path}:{line}")

          new_in_pr = []
          preexisting = []

          for a in pr_alerts:
              rule_id = a.get("rule", {}).get("id", "")
              loc = a.get("most_recent_instance", {}).get("location", {})
              path = loc.get("path", "")
              line = loc.get("start_line", 0)
              key = f"{rule_id}:{path}:{line}"

              alert_info = {
                  "number": a.get("number"),
                  "rule_id": rule_id,
                  "rule_description": a.get("rule", {}).get("description", rule_id),
                  "severity": a.get("rule", {}).get("security_severity_level", "unknown"),
                  "file": path,
                  "start_line": line,
                  "end_line": loc.get("end_line", line),
                  "message": a.get("most_recent_instance", {}).get("message", {}).get("text", ""),
                  "key": key
              }

              if key in main_rule_file_set:
                  preexisting.append(alert_info)
              else:
                  new_in_pr.append(alert_info)

          severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3, "unknown": 4}
          new_in_pr.sort(key=lambda a: severity_order.get(a["severity"], 4))
          preexisting.sort(key=lambda a: severity_order.get(a["severity"], 4))

          with open("/tmp/new_in_pr_alerts.json", "w") as f:
              json.dump(new_in_pr, f, indent=2)
          with open("/tmp/preexisting_alerts.json", "w") as f:
              json.dump(preexisting, f, indent=2)

          new_summary = []
          for a in new_in_pr:
              new_summary.append(f"- [{a['rule_id']}] {a['rule_description']} at {a['file']}:{a['start_line']} (severity: {a['severity']})")
          with open("/tmp/new_in_pr_summary.txt", "w") as f:
              f.write("\n".join(new_summary) if new_summary else "None")

          pre_summary = []
          for a in preexisting:
              pre_summary.append(f"- [{a['rule_id']}] {a['rule_description']} at {a['file']}:{a['start_line']} (severity: {a['severity']})")
          with open("/tmp/preexisting_summary.txt", "w") as f:
              f.write("\n".join(pre_summary) if pre_summary else "None")

          total = len(new_in_pr) + len(preexisting)
          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"total_count={total}\n")
              f.write(f"new_in_pr_count={len(new_in_pr)}\n")
              f.write(f"preexisting_count={len(preexisting)}\n")

          print(f"\nClassification results:")
          print(f"  New in PR: {len(new_in_pr)}")
          for a in new_in_pr:
              print(f"    - [{a['rule_id']}] {a['file']}:{a['start_line']} ({a['severity']})")
          print(f"  Pre-existing: {len(preexisting)}")
          for a in preexisting:
              print(f"    - [{a['rule_id']}] {a['file']}:{a['start_line']} ({a['severity']})")
          CLASSIFY_EOF

      # ------------------------------------------------------------------
      # STEP 4: Load previous attempt history from the existing PR comment.
      # We search for our marker comment and extract the hidden attempt
      # counters. This lets us skip alerts that have already been tried
      # (max 2 attempts) and prevents the infinite retry loop.
      # ------------------------------------------------------------------
      - name: Load attempt history from existing PR comment
        id: load-history
        if: steps.classify-alerts.outputs.total_count != '0'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          PR_NUMBER: ${{ steps.pr-context.outputs.pr_number }}
        run: |
          REPO="${{ github.repository }}"
          PR_NUMBER="$PR_NUMBER"

          # Fetch ALL comments with pagination (enterprise PRs can have >100 comments)
          PAGE=1
          echo '[]' > /tmp/pr_comments.json
          while true; do
            curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$PR_NUMBER/comments?per_page=100&page=$PAGE" \
              > /tmp/pr_comments_page.json
            PAGE_COUNT=$(jq 'if type == "array" then length else 0 end' /tmp/pr_comments_page.json)
            if [ "$PAGE_COUNT" -eq 0 ]; then
              break
            fi
            jq -s '.[0] + .[1]' /tmp/pr_comments.json /tmp/pr_comments_page.json > /tmp/pr_comments_merged.json
            mv /tmp/pr_comments_merged.json /tmp/pr_comments.json
            if [ "$PAGE_COUNT" -lt 100 ]; then
              break
            fi
            PAGE=$((PAGE + 1))
          done
          echo "Fetched $(jq length /tmp/pr_comments.json) total comments (across $PAGE page(s))"

          # Parse from file — safe regardless of comment content
          python3 << 'HISTORY_EOF'
          import json, os, re

          with open("/tmp/pr_comments.json") as f:
              try:
                  comments = json.load(f)
                  if not isinstance(comments, list):
                      comments = []
              except Exception:
                  comments = []

          print(f"Loaded {len(comments)} comments from API")

          marker = '<!-- devin-security-review -->'
          comment_id = ''
          attempts = {}
          unfixable = set()

          for c in comments:
              body = c.get('body', '')
              if marker in body:
                  comment_id = str(c.get('id', ''))
                  attempts = {}
                  unfixable = set()
                  m = re.search(r'<!-- attempts:(.*?) -->', body)
                  if m:
                      for pair in m.group(1).split(','):
                          if '=' in pair:
                              k, v = pair.rsplit('=', 1)
                              try:
                                  attempts[k.strip()] = int(v.strip())
                              except ValueError:
                                  pass
                  m = re.search(r'<!-- unfixable:(.*?) -->', body)
                  if m:
                      for k in m.group(1).split(','):
                          if k.strip():
                              unfixable.add(k.strip())

          gh_out = os.environ.get('GITHUB_OUTPUT', '/dev/null')
          with open(gh_out, 'a') as f:
              f.write(f'comment_id={comment_id}\n')

          with open('/tmp/attempt_history.json', 'w') as f:
              json.dump({'attempts': attempts, 'unfixable': list(unfixable), 'comment_id': comment_id}, f)

          print(f'Found existing comment: {"yes (id=" + comment_id + ")" if comment_id else "no"}')
          print(f'Attempt history: {len(attempts)} alerts tracked')
          print(f'Unfixable alerts: {len(unfixable)}')
          for k, v in attempts.items():
              print(f'  {k}: {v} attempt(s)')
          for k in unfixable:
              print(f'  [unfixable] {k}')
          HISTORY_EOF

      # ------------------------------------------------------------------
      # STEP 5: Filter alerts — skip already-attempted (>=2) and unfixable.
      # This is the core circuit breaker that prevents infinite loops.
      # Alerts that have been tried twice are moved to the "unfixable" list
      # and will appear in the "REQUIRES MANUAL REVIEW" section.
      # ------------------------------------------------------------------
      - name: Filter alerts (skip already-attempted, mark unfixable)
        id: filter-alerts
        if: steps.classify-alerts.outputs.total_count != '0'
        run: |
          IS_DEVIN_FIX="${{ steps.detect-devin.outputs.is_devin_fix }}"

          python3 << 'FILTER_EOF'
          import json, os

          MAX_ATTEMPTS = 2
          is_devin_fix = os.environ.get("IS_DEVIN_FIX", "false") == "true"

          with open("/tmp/attempt_history.json") as f:
              history = json.load(f)
          attempts = history.get("attempts", {})
          unfixable = set(history.get("unfixable", []))

          with open("/tmp/new_in_pr_alerts.json") as f:
              new_alerts = json.load(f)
          with open("/tmp/preexisting_alerts.json") as f:
              pre_alerts = json.load(f)

          def filter_alerts(alerts, label):
              actionable = []
              skipped_unfixable = []
              newly_unfixable = []
              for a in alerts:
                  key = a.get("key", f"{a['rule_id']}:{a['file']}:{a['start_line']}")
                  attempt_count = attempts.get(key, 0)

                  if key in unfixable:
                      skipped_unfixable.append(a)
                      print(f"  [SKIP-UNFIXABLE] {key} — already marked unfixable")
                      continue

                  if attempt_count >= MAX_ATTEMPTS:
                      newly_unfixable.append(a)
                      unfixable.add(key)
                      print(f"  [NOW-UNFIXABLE] {key} — {attempt_count} attempts exhausted")
                      continue

                  # If this is a Devin fix re-trigger and the alert was already
                  # attempted once, give it one more chance (attempt 2).
                  # But if it already had 1+ attempts, we still process it —
                  # the max check above handles the cutoff.
                  actionable.append(a)
                  attempts[key] = attempt_count + 1
                  print(f"  [PROCESS] {key} — attempt {attempt_count + 1}/{MAX_ATTEMPTS}")

              return actionable, skipped_unfixable, newly_unfixable

          print(f"Devin fix re-trigger: {is_devin_fix}")
          print(f"\nFiltering new-in-PR alerts:")
          new_actionable, new_skipped, new_newly_unfixable = filter_alerts(new_alerts, "new")
          print(f"\nFiltering pre-existing alerts:")
          pre_actionable, pre_skipped, pre_newly_unfixable = filter_alerts(pre_alerts, "pre")

          all_unfixable = list(unfixable)
          all_unfixable_alerts = new_skipped + new_newly_unfixable + pre_skipped + pre_newly_unfixable

          current_alert_keys = set()
          for a in new_alerts + pre_alerts:
              current_alert_keys.add(a.get("key", f"{a['rule_id']}:{a['file']}:{a['start_line']}"))

          fixed_keys = []
          for prev_key in history.get("attempts", {}):
              if prev_key not in current_alert_keys and prev_key not in unfixable:
                  fixed_keys.append(prev_key)
                  print(f"  [FIXED] {prev_key} — no longer in CodeQL alerts")

          with open("/tmp/new_in_pr_actionable.json", "w") as f:
              json.dump(new_actionable, f, indent=2)
          with open("/tmp/preexisting_actionable.json", "w") as f:
              json.dump(pre_actionable, f, indent=2)
          with open("/tmp/unfixable_alerts.json", "w") as f:
              json.dump(all_unfixable_alerts, f, indent=2)
          with open("/tmp/attempt_history_updated.json", "w") as f:
              json.dump({"attempts": attempts, "unfixable": all_unfixable, "fixed": fixed_keys}, f)

          new_actionable_summary = []
          for a in new_actionable:
              new_actionable_summary.append(
                  f"- [{a['rule_id']}] {a['rule_description']} at {a['file']}:{a['start_line']} (severity: {a['severity']})"
              )
          with open("/tmp/new_actionable_summary.txt", "w") as f:
              f.write("\n".join(new_actionable_summary) if new_actionable_summary else "None")

          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"new_actionable_count={len(new_actionable)}\n")
              f.write(f"pre_actionable_count={len(pre_actionable)}\n")
              f.write(f"unfixable_count={len(all_unfixable)}\n")

          debug_filter = {
              "is_devin_fix": is_devin_fix,
              "new_total": len(new_alerts),
              "pre_total": len(pre_alerts),
              "new_actionable": len(new_actionable),
              "pre_actionable": len(pre_actionable),
              "new_skipped_unfixable": len(new_skipped),
              "new_newly_unfixable": len(new_newly_unfixable),
              "pre_skipped_unfixable": len(pre_skipped),
              "pre_newly_unfixable": len(pre_newly_unfixable),
              "total_unfixable": len(all_unfixable),
              "attempt_history": attempts,
              "unfixable_keys": all_unfixable,
              "fixed_keys": fixed_keys,
          }
          with open("/tmp/debug_filter.json", "w") as f:
              json.dump(debug_filter, f, indent=2)

          print(f"\nSummary:")
          print(f"  New-in-PR actionable: {len(new_actionable)}")
          print(f"  Pre-existing actionable: {len(pre_actionable)}")
          print(f"  Unfixable (skipped): {len(all_unfixable)}")
          print(f"  Fixed (resolved): {len(fixed_keys)}")
          FILTER_EOF

      # ------------------------------------------------------------------
      # STEP 6: Create Devin session for new-in-PR alerts.
      # Enhanced prompt includes:
      # - CodeQL CLI local verification (same config as project)
      # - Max 2 local fix attempts per alert before marking unfixable
      # - git pull --rebase before push (prevents merge commits)
      # - Skip unfixable alerts and continue with remaining batch
      # - idempotent=true to prevent duplicate sessions
      # - max_acu_limit to cap resource usage
      # ------------------------------------------------------------------
      - name: Create Devin session for new-in-PR alerts
        if: steps.filter-alerts.outputs.new_actionable_count != '0' && steps.health-check.outputs.devin_available == 'true'
        id: devin-new-pr
        env:
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
          PR_NUMBER: ${{ steps.pr-context.outputs.pr_number }}
          PR_BRANCH: ${{ steps.pr-context.outputs.head_branch }}
        run: |
          REPO="${{ github.repository }}"
          PR_NUMBER="$PR_NUMBER"
          PR_BRANCH="$PR_BRANCH"
          ALERT_SUMMARY=$(cat /tmp/new_actionable_summary.txt)
          ALERT_DETAILS=$(cat /tmp/new_in_pr_actionable.json)

          PROMPT=$(cat <<PROMPT_EOF
          You are a security engineer fixing vulnerabilities found by CodeQL in PR #${PR_NUMBER} of repository ${REPO}.

          These alerts were INTRODUCED by this PR on branch '${PR_BRANCH}':

          ${ALERT_SUMMARY}

          Detailed alert information:
          ${ALERT_DETAILS}

          Instructions:
          1. Clone the repository: https://github.com/${REPO}.git
          2. Checkout branch '${PR_BRANCH}'
          3. Pull latest changes: git pull --rebase origin '${PR_BRANCH}' (ALWAYS rebase, never merge — keeps git history clean)
          4. For each alert, read the surrounding code and understand the full context
          5. Fix each alert ONE AT A TIME. For each alert:
             a. Apply a minimal, focused fix following the codebase's existing conventions
             b. VERIFY the fix locally using CodeQL CLI before committing:
                - Install CodeQL CLI: wget -q https://github.com/github/codeql-cli-binaries/releases/latest/download/codeql-linux64.zip && unzip -q codeql-linux64.zip
                - Create database: ./codeql/codeql database create /tmp/codeql-db --language=python --source-root=. --overwrite
                - Run analysis: ./codeql/codeql database analyze /tmp/codeql-db codeql/python-queries:codeql-suites/python-security-and-quality.qls --format=sarif-latest --output=/tmp/results.sarif --download
                - Check if the specific alert rule ID still appears in /tmp/results.sarif for the same file and line
             c. If the alert STILL appears after your fix, revise the fix and re-run CodeQL (max 2 attempts)
             d. If after 2 attempts the alert persists, SKIP this alert — do NOT commit the broken fix. Note it as unfixable.
             e. If the fix resolves the alert, commit with message: fix: [rule_id] description (file:line)
          6. Conventions to follow:
             - If the project uses an ORM, use parameterized queries via the ORM
             - If there are existing sanitization utilities, reuse them
             - Do not introduce new dependencies unless absolutely necessary
          7. If the repository has tests, run them to ensure no regressions
          8. Before pushing, run: git pull --rebase origin '${PR_BRANCH}' (ALWAYS rebase, never merge)
          9. Push all commit(s) to branch '${PR_BRANCH}' (use --force-with-lease if rebase changed history)

          IMPORTANT:
          - Each security issue MUST be a separate commit
          - Do NOT just suppress or ignore alerts — fix the root cause
          - Do NOT create a new PR — push directly to the existing branch '${PR_BRANCH}'
          - Keep fixes minimal and surgical — do not refactor unrelated code
          - If an alert cannot be fixed after 2 CodeQL verification attempts, SKIP it and move to the next one
          - At the end, report which alerts were fixed and which were unfixable
          PROMPT_EOF
          )

          PROMPT_JSON=$(echo "$PROMPT" | python3 -c "import sys,json; print(json.dumps(sys.stdin.read()))")

          # Create session with idempotent flag and ACU limit
          RESPONSE=$(curl -s -w "\n%{http_code}" \
            -X POST "https://api.devin.ai/v1/sessions" \
            -H "Authorization: Bearer $DEVIN_API_KEY" \
            -H "Content-Type: application/json" \
            -d "{\"prompt\": $PROMPT_JSON, \"idempotent\": true, \"max_acu_limit\": 10}")

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | head -n -1)

          echo "Devin API response (HTTP $HTTP_CODE):"
          echo "$BODY" | jq . 2>/dev/null || echo "$BODY"

          # Handle rate limiting with exponential backoff
          if [ "$HTTP_CODE" = "429" ]; then
            echo "Rate limited. Waiting 60s and retrying..."
            sleep 60
            RESPONSE=$(curl -s -w "\n%{http_code}" \
              -X POST "https://api.devin.ai/v1/sessions" \
              -H "Authorization: Bearer $DEVIN_API_KEY" \
              -H "Content-Type: application/json" \
              -d "{\"prompt\": $PROMPT_JSON, \"idempotent\": true, \"max_acu_limit\": 10}")
            HTTP_CODE=$(echo "$RESPONSE" | tail -1)
            BODY=$(echo "$RESPONSE" | head -n -1)
            echo "Retry response (HTTP $HTTP_CODE):"
            echo "$BODY" | jq . 2>/dev/null || echo "$BODY"
          fi

          SESSION_ID=$(echo "$BODY" | jq -r '.session_id // empty')
          SESSION_URL=$(echo "$BODY" | jq -r '.url // empty')
          if [ -z "$SESSION_URL" ] && [ -n "$SESSION_ID" ]; then
            SESSION_URL="https://app.devin.ai/sessions/${SESSION_ID}"
          fi

          if [ -z "$SESSION_ID" ]; then
            echo "::error::Failed to create Devin session for new-in-PR alerts (HTTP $HTTP_CODE)"
            echo "session_created=false" >> $GITHUB_OUTPUT
            echo "session_failed=true" >> $GITHUB_OUTPUT
          else
            echo "Devin session created: $SESSION_URL"
            echo "session_id=$SESSION_ID" >> $GITHUB_OUTPUT
            echo "session_url=$SESSION_URL" >> $GITHUB_OUTPUT
            echo "session_created=true" >> $GITHUB_OUTPUT
          fi

      # ------------------------------------------------------------------
      # STEP 7: Create Devin sessions for pre-existing alerts (batched).
      # Same enhancements as new-in-PR: CodeQL verification, rebase,
      # skip unfixable, rate limit handling, idempotent, ACU cap.
      # Batching: prefer same-file, backfill to fill batch (cap 15).
      #
      # IMPORTANT: Branch name is DETERMINISTIC (based on PR number, not
      # timestamp) so that re-runs reuse the same branch and don't create
      # infinite new branches/PRs. The idempotent flag on sessions also
      # prevents duplicate sessions for the same prompt.
      # ------------------------------------------------------------------
      - name: Create Devin sessions for pre-existing alerts (batched)
        if: steps.filter-alerts.outputs.pre_actionable_count != '0' && steps.health-check.outputs.devin_available == 'true'
        id: devin-preexisting
        env:
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
          PR_NUMBER: ${{ steps.pr-context.outputs.pr_number }}
        run: |
          export REPO="${{ github.repository }}"
          export PR_NUMBER="$PR_NUMBER"
          export FIX_BRANCH="devin/security-fixes-pr${PR_NUMBER}"

          echo "fix_branch=$FIX_BRANCH" >> $GITHUB_OUTPUT

          python3 << 'BATCH_EOF'
          import json, os, time, urllib.request, urllib.error

          alerts = json.load(open("/tmp/preexisting_actionable.json"))
          repo = os.environ["REPO"]
          fix_branch = os.environ["FIX_BRANCH"]
          pr_number = os.environ["PR_NUMBER"]
          api_key = os.environ["DEVIN_API_KEY"]
          max_per_batch = 15
          max_concurrent = 3
          max_total_sessions = 20

          severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3, "unknown": 4}

          # Group by file, sort each group by severity
          file_groups = {}
          for a in alerts:
              file_groups.setdefault(a["file"], []).append(a)
          for fg in file_groups.values():
              fg.sort(key=lambda a: severity_order.get(a["severity"], 4))

          # Sort files by most severe alert first
          sorted_files = sorted(
              file_groups.items(),
              key=lambda kv: min(severity_order.get(a["severity"], 4) for a in kv[1])
          )

          # Build batches: prefer same-file, backfill remaining space
          batches = []
          remaining = {}
          for fp, fa in sorted_files:
              remaining[fp] = list(fa)

          for fp, fa in sorted_files:
              if not remaining[fp]:
                  continue
              while remaining[fp]:
                  batch = remaining[fp][:max_per_batch]
                  remaining[fp] = remaining[fp][len(batch):]
                  if len(batch) < max_per_batch:
                      for other_fp, other_fa in sorted_files:
                          if other_fp == fp or not remaining.get(other_fp):
                              continue
                          space = max_per_batch - len(batch)
                          taken = remaining[other_fp][:space]
                          remaining[other_fp] = remaining[other_fp][len(taken):]
                          batch.extend(taken)
                          if len(batch) >= max_per_batch:
                              break
                  batches.append(batch)

          session_ids = []
          session_urls = []

          for i, batch in enumerate(batches[:max_total_sessions]):
              summary_lines = []
              for a in batch:
                  summary_lines.append(
                      f"- [{a['rule_id']}] {a['rule_description']} at {a['file']}:{a['start_line']} (severity: {a['severity']})"
                  )
              summary = "\n".join(summary_lines)
              details = json.dumps(batch, indent=2)

              prompt = (
                  f"You are a security engineer fixing pre-existing vulnerabilities found by CodeQL in repository {repo}.\n\n"
                  f"These alerts exist on the main branch and were detected during review of PR #{pr_number}:\n\n"
                  f"{summary}\n\n"
                  f"Detailed alert information:\n{details}\n\n"
                  "Instructions:\n"
                  f"1. Clone the repository: https://github.com/{repo}.git\n"
                  f"2. Create and checkout branch '{fix_branch}' from main (or pull --rebase if it already exists)\n"
                  "3. For each alert, read the surrounding code and understand the full context\n"
                  "4. Fix each alert ONE AT A TIME. For each alert:\n"
                  "   a. Apply a minimal, focused fix following the codebase's existing conventions\n"
                  "   b. VERIFY the fix locally using CodeQL CLI before committing:\n"
                  "      - Install CodeQL CLI: wget -q https://github.com/github/codeql-cli-binaries/releases/latest/download/codeql-linux64.zip && unzip -q codeql-linux64.zip\n"
                  "      - Create database: ./codeql/codeql database create /tmp/codeql-db --language=python --source-root=. --overwrite\n"
                  "      - Run analysis: ./codeql/codeql database analyze /tmp/codeql-db codeql/python-queries:codeql-suites/python-security-and-quality.qls --format=sarif-latest --output=/tmp/results.sarif --download\n"
                  "      - Check if the specific alert rule ID still appears for the same file and line\n"
                  "   c. If the alert STILL appears after your fix, revise and re-run CodeQL (max 2 attempts)\n"
                  "   d. If after 2 attempts the alert persists, SKIP it - do NOT commit a broken fix. Note it as unfixable.\n"
                  "   e. If the fix resolves the alert, commit with message: fix: [rule_id] description (file:line)\n"
                  "5. Conventions to follow:\n"
                  "   - If the project uses an ORM, use parameterized queries via the ORM\n"
                  "   - If there are existing sanitization utilities, reuse them\n"
                  "   - Do not introduce new dependencies unless absolutely necessary\n"
                  "6. If the repository has tests, run them to ensure no regressions\n"
                  f"7. Before pushing, run: git pull --rebase origin '{fix_branch}' (handle conflicts if any)\n"
                  f"8. Push all commit(s) to branch '{fix_branch}'\n\n"
                  "IMPORTANT:\n"
                  "- Each security issue MUST be a separate commit\n"
                  "- Do NOT just suppress or ignore alerts - fix the root cause\n"
                  f"- Do NOT create a new PR - just push to branch '{fix_branch}'\n"
                  "- Keep fixes minimal and surgical - do not refactor unrelated code\n"
                  "- If an alert cannot be fixed after 2 CodeQL verification attempts, SKIP it and continue\n"
                  "- At the end, report which alerts were fixed and which were unfixable"
              )

              body_bytes = json.dumps({
                  "prompt": prompt,
                  "idempotent": True,
                  "max_acu_limit": 10
              }).encode()

              req = urllib.request.Request(
                  "https://api.devin.ai/v1/sessions",
                  data=body_bytes,
                  headers={
                      "Authorization": f"Bearer {api_key}",
                      "Content-Type": "application/json"
                  },
                  method="POST"
              )

              # Retry with exponential backoff on rate limits
              max_retries = 3
              for attempt in range(max_retries):
                  try:
                      resp = urllib.request.urlopen(req)
                      result = json.loads(resp.read())
                      sid = result.get("session_id", "")
                      if sid:
                          session_ids.append(sid)
                          surl = result.get("url", f"https://app.devin.ai/sessions/{sid}")
                          session_urls.append(surl)
                          print(f"Batch {i+1}/{min(len(batches), max_total_sessions)}: Session {sid} created")
                      else:
                          print(f"Batch {i+1}: Failed - {result}")
                      break
                  except urllib.error.HTTPError as e:
                      if e.code == 429 and attempt < max_retries - 1:
                          wait_time = 30 * (2 ** attempt)
                          print(f"Batch {i+1}: Rate limited (429). Waiting {wait_time}s...")
                          time.sleep(wait_time)
                      else:
                          print(f"Batch {i+1}: Error {e.code} - {e.read().decode()[:200]}")
                          break
                  except Exception as e:
                      print(f"Batch {i+1}: Error - {e}")
                      break

              # Pause between concurrent waves
              if (i + 1) % max_concurrent == 0 and i + 1 < len(batches):
                  print(f"Concurrency pause (30s)...")
                  time.sleep(30)

          with open("/tmp/preexisting_session_ids.txt", "w") as f:
              f.write("\n".join(session_ids))
          with open("/tmp/preexisting_session_urls.txt", "w") as f:
              f.write("\n".join(session_urls))

          print(f"\nCreated {len(session_ids)} session(s) for {len(alerts)} pre-existing alert(s)")
          BATCH_EOF

          SESSION_COUNT=$(wc -l < /tmp/preexisting_session_ids.txt 2>/dev/null || echo "0")
          FIRST_URL=$(head -1 /tmp/preexisting_session_urls.txt 2>/dev/null || echo "")
          echo "session_count=$SESSION_COUNT" >> $GITHUB_OUTPUT
          echo "first_session_url=$FIRST_URL" >> $GITHUB_OUTPUT
          echo "sessions_created=true" >> $GITHUB_OUTPUT

      # ------------------------------------------------------------------
      # STEP 8: Find-or-update the PR comment.
      # Uses a hidden HTML marker to find the existing bot comment.
      # If found, PATCHes it in place (no flood). If not, POSTs new one.
      # Comment includes:
      #   - Alert summary with severity table
      #   - Devin session links and commits/files tabs
      #   - Unfixable alerts section (REQUIRES MANUAL REVIEW)
      #   - Safe-to-merge signal
      #   - Hidden attempt counters for next run
      # ------------------------------------------------------------------
      - name: Post or update PR comment with results
        if: steps.classify-alerts.outputs.total_count != '0'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          PR_NUMBER: ${{ steps.pr-context.outputs.pr_number }}
        run: |
          export REPO="${{ github.repository }}"
          export PR_NUMBER="$PR_NUMBER"
          export TOTAL_COUNT="${{ steps.classify-alerts.outputs.total_count }}"
          export NEW_COUNT="${{ steps.classify-alerts.outputs.new_in_pr_count }}"
          export PRE_COUNT="${{ steps.classify-alerts.outputs.preexisting_count }}"
          export NEW_ACTIONABLE="${{ steps.filter-alerts.outputs.new_actionable_count }}"
          export PRE_ACTIONABLE="${{ steps.filter-alerts.outputs.pre_actionable_count }}"
          export UNFIXABLE_COUNT="${{ steps.filter-alerts.outputs.unfixable_count }}"
          export NEW_SESSION_URL="${{ steps.devin-new-pr.outputs.session_url }}"
          export NEW_SESSION_CREATED="${{ steps.devin-new-pr.outputs.session_created }}"
          export PRE_SESSIONS_CREATED="${{ steps.devin-preexisting.outputs.sessions_created }}"
          export PRE_FIRST_URL="${{ steps.devin-preexisting.outputs.first_session_url }}"
          export PRE_SESSION_COUNT="${{ steps.devin-preexisting.outputs.session_count }}"
          export FIX_BRANCH="${{ steps.devin-preexisting.outputs.fix_branch }}"
          export EXISTING_COMMENT_ID="${{ steps.load-history.outputs.comment_id }}"
          export DEVIN_AVAILABLE="${{ steps.health-check.outputs.devin_available }}"
          export GITHUB_RUN_ID="${{ github.run_id }}"
          export GITHUB_SERVER_URL="${{ github.server_url }}"
          export DEBUG_MODE="${{ env.DEBUG }}"
          export IS_DEVIN_FIX="${{ steps.detect-devin.outputs.is_devin_fix }}"

          export NEW_SUMMARY=$(cat /tmp/new_in_pr_summary.txt 2>/dev/null || echo "None")
          export PRE_SUMMARY=$(cat /tmp/preexisting_summary.txt 2>/dev/null || echo "None")

          # Build the comment body using Python for cleaner string handling
          python3 << 'COMMENT_EOF'
          import json, os, datetime

          repo = os.environ["REPO"]
          pr_number = os.environ["PR_NUMBER"]
          total = os.environ.get("TOTAL_COUNT", "0")
          new_count = os.environ.get("NEW_COUNT", "0")
          pre_count = os.environ.get("PRE_COUNT", "0")
          new_actionable = os.environ.get("NEW_ACTIONABLE", "0")
          pre_actionable = os.environ.get("PRE_ACTIONABLE", "0")
          unfixable_count = os.environ.get("UNFIXABLE_COUNT", "0")
          new_session_url = os.environ.get("NEW_SESSION_URL", "")
          new_session_created = os.environ.get("NEW_SESSION_CREATED", "false")
          pre_sessions_created = os.environ.get("PRE_SESSIONS_CREATED", "false")
          pre_first_url = os.environ.get("PRE_FIRST_URL", "")
          pre_session_count = os.environ.get("PRE_SESSION_COUNT", "0")
          fix_branch = os.environ.get("FIX_BRANCH", "")
          existing_comment_id = os.environ.get("EXISTING_COMMENT_ID", "")
          new_summary = os.environ.get("NEW_SUMMARY", "None")
          pre_summary = os.environ.get("PRE_SUMMARY", "None")
          devin_available = os.environ.get("DEVIN_AVAILABLE", "true")
          server_url = os.environ.get("GITHUB_SERVER_URL", "https://github.com")
          run_id = os.environ.get("GITHUB_RUN_ID", "")
          debug_mode = os.environ.get("DEBUG_MODE", "false") == "true"
          is_devin_fix = os.environ.get("IS_DEVIN_FIX", "false")
          pr_url = f"{server_url}/{repo}/pull/{pr_number}"

          # Load attempt history for hidden markers
          try:
              with open("/tmp/attempt_history_updated.json") as f:
                  history = json.load(f)
          except Exception:
              history = {"attempts": {}, "unfixable": []}

          # Load unfixable alert details for display
          try:
              with open("/tmp/unfixable_alerts.json") as f:
                  unfixable_alerts = json.load(f)
          except Exception:
              unfixable_alerts = []

          # Load all alerts for per-alert status table
          try:
              with open("/tmp/new_in_pr_alerts.json") as f:
                  all_new_alerts = json.load(f)
          except Exception:
              all_new_alerts = []
          try:
              with open("/tmp/preexisting_alerts.json") as f:
                  all_pre_alerts = json.load(f)
          except Exception:
              all_pre_alerts = []

          fixed_keys = history.get("fixed", [])
          unfixable_set = set(history.get("unfixable", []))

          # --- Build comment body ---
          lines = []
          lines.append("<!-- devin-security-review -->")
          lines.append("## Devin Security Review\n")
          lines.append(f"**Total CodeQL Alerts: {total}**\n")

          # Degraded mode banner (Devin API unavailable)
          if devin_available != "true":
              lines.append("> **DEGRADED MODE** — Devin API is currently unavailable. Alerts are listed below but cannot be auto-fixed.\n")
              lines.append("> **Your code is still protected.** CodeQL's required status check blocks merging of PRs with unresolved security alerts. Fix these issues manually or wait for Devin to come back online and re-run this workflow.\n")

          # Status signal (safe-to-merge vs needs manual review)
          if int(unfixable_count) > 0:
              lines.append(f"> **REQUIRES MANUAL REVIEW** — {unfixable_count} alert(s) could not be auto-fixed after multiple attempts. See details below.\n")
          elif devin_available == "true" and new_actionable == "0" and pre_actionable == "0" and int(total) > 0:
              lines.append("> **All alerts are being handled** — Devin sessions are working on fixes or alerts have been resolved.\n")

          # New-in-PR section
          if new_count != "0" and new_count:
              lines.append(f"### New Alerts (introduced in this PR): {new_count}\n")
              lines.append("```")
              lines.append(new_summary)
              lines.append("```\n")
              if new_session_created == "true" and new_session_url:
                  lines.append("**Devin is fixing these issues.** Each fix will appear as a separate commit on this PR.\n")
                  lines.append("| What | Link |")
                  lines.append("|------|------|")
                  lines.append(f"| Watch Devin work | [Devin Session]({new_session_url}) |")
                  lines.append(f"| See fix commits | [Commits tab]({pr_url}/commits) |")
                  lines.append(f"| Review all changes | [Files changed]({pr_url}/files) |")
                  lines.append("")
              elif devin_available != "true":
                  lines.append("**Devin API is unavailable.** Alerts listed above require manual fixing or wait for Devin to come back online.\n")
              elif new_actionable == "0":
                  lines.append("All new alerts have been attempted (max retries reached). See unfixable section below.\n")
              elif new_session_created == "false":
                  lines.append("**Session creation failed.** Alerts listed above require manual attention. This workflow run has been marked as failed.\n")

              # Per-alert status table
              lines.append("| Severity | Rule | File | Status | Attempts |")
              lines.append("|----------|------|------|--------|----------|")
              for a in all_new_alerts:
                  key = a.get("key", f"{a['rule_id']}:{a['file']}:{a['start_line']}")
                  att = history["attempts"].get(key, 0)
                  if key in unfixable_set:
                      status = "Needs Manual Fix"
                  elif att >= 2:
                      status = "Needs Manual Fix"
                  elif att > 0:
                      status = "In Progress"
                  else:
                      status = "Queued"
                  lines.append(f"| {a['severity']} | `{a['rule_id']}` | `{a['file']}:{a['start_line']}` | **{status}** | {att}/2 |")
              # Show fixed alerts from previous runs
              for fk in fixed_keys:
                  parts = fk.split(":")
                  if len(parts) >= 3:
                      rule = parts[0]
                      fpath = ":".join(parts[1:-1])
                      fline = parts[-1]
                      att = history["attempts"].get(fk, 1)
                      lines.append(f"| - | `{rule}` | `{fpath}:{fline}` | **Fixed** | {att}/2 |")
              lines.append("")

          # Pre-existing section
          if pre_count != "0" and pre_count:
              lines.append(f"### Pre-existing Alerts (already on main): {pre_count}\n")
              lines.append("These exist on main and were not introduced by this PR.\n")
              if pre_sessions_created == "true":
                  lines.append(f"**Devin is fixing these on branch `{fix_branch}`.** A separate PR will be created.\n")
                  lines.append("| Session | Link |")
                  lines.append("|---------|------|")
                  try:
                      with open("/tmp/preexisting_session_urls.txt") as f:
                          for j, url in enumerate(f.read().strip().split("\n"), 1):
                              if url:
                                  lines.append(f"| Batch {j} | [Watch Devin work]({url}) |")
                  except Exception:
                      pass
                  lines.append("")

              # Per-alert status table for pre-existing
              lines.append("| Severity | Rule | File | Status | Attempts |")
              lines.append("|----------|------|------|--------|----------|")
              for a in all_pre_alerts:
                  key = a.get("key", f"{a['rule_id']}:{a['file']}:{a['start_line']}")
                  att = history["attempts"].get(key, 0)
                  if key in unfixable_set:
                      status = "Needs Manual Fix"
                  elif att >= 2:
                      status = "Needs Manual Fix"
                  elif att > 0:
                      status = "In Progress"
                  else:
                      status = "Queued"
                  lines.append(f"| {a['severity']} | `{a['rule_id']}` | `{a['file']}:{a['start_line']}` | **{status}** | {att}/2 |")
              lines.append("")

          # Unfixable alerts section (critical for developer visibility)
          if unfixable_alerts:
              lines.append("### Unfixable Alerts (REQUIRES MANUAL REVIEW)\n")
              lines.append("These alerts could not be automatically fixed after 2 attempts. A developer must review and fix these manually.\n")
              lines.append("**To find these in CodeQL:** Filter alerts by `is:open` in the Security tab → Code scanning.\n")
              lines.append("| Severity | Rule | File | Attempts |")
              lines.append("|----------|------|------|----------|")
              for a in unfixable_alerts:
                  key = a.get("key", f"{a['rule_id']}:{a['file']}:{a['start_line']}")
                  att = history["attempts"].get(key, 2)
                  lines.append(f"| {a['severity']} | `{a['rule_id']}` | `{a['file']}:{a['start_line']}` | {att} |")
              lines.append("")

          # DEBUG section (verbose internal workflow details)
          if debug_mode:
              lines.append("")
              lines.append("<details>")
              lines.append("<summary><strong>Debug Log</strong> (workflow internals)</summary>")
              lines.append("")
              lines.append(f"**Timestamp**: {datetime.datetime.now(datetime.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}")
              lines.append(f"**Workflow Run**: [{run_id}]({server_url}/{repo}/actions/runs/{run_id})")
              lines.append(f"**Devin-commit detected**: `{is_devin_fix}`")
              lines.append(f"**Devin API available**: `{devin_available}`")
              lines.append("")

              lines.append("#### Health Check")
              try:
                  with open("/tmp/debug_health.json") as f:
                      hc = json.load(f)
                  lines.append(f"- GH_PAT valid: `{hc.get('gh_pat_valid', 'unknown')}`")
                  lines.append(f"- Devin API reachable: `{hc.get('devin_available', 'unknown')}`")
              except Exception:
                  lines.append("- Health check data not available")
              lines.append("")

              lines.append("#### Alert Filtering")
              try:
                  with open("/tmp/debug_filter.json") as f:
                      df = json.load(f)
                  lines.append(f"- New alerts (total/actionable): `{df.get('new_total', '?')}/{df.get('new_actionable', '?')}`")
                  lines.append(f"- Pre-existing alerts (total/actionable): `{df.get('pre_total', '?')}/{df.get('pre_actionable', '?')}`")
                  lines.append(f"- Newly marked unfixable: `{df.get('new_newly_unfixable', 0) + df.get('pre_newly_unfixable', 0)}`")
                  lines.append(f"- Previously unfixable (skipped): `{df.get('new_skipped_unfixable', 0) + df.get('pre_skipped_unfixable', 0)}`")
                  ah = df.get("attempt_history", {})
                  if ah:
                      lines.append("")
                      lines.append("#### Attempt History")
                      lines.append("| Alert Key | Attempts | Status |")
                      lines.append("|-----------|----------|--------|")
                      uf_keys = set(df.get("unfixable_keys", []))
                      for k, v in ah.items():
                          status = "unfixable" if k in uf_keys else ("retry pending" if v < 2 else "max reached")
                          lines.append(f"| `{k}` | {v}/2 | {status} |")
              except Exception:
                  lines.append("- Filter data not available")
              lines.append("")

              lines.append("#### Devin Sessions")
              if new_session_created == "true" and new_session_url:
                  lines.append(f"- New-in-PR session: [{new_session_url}]({new_session_url})")
              elif devin_available == "true" and new_actionable != "0":
                  lines.append("- New-in-PR session: **failed to create** (check workflow logs)")
              else:
                  lines.append(f"- New-in-PR session: not needed (actionable={new_actionable}, devin_available={devin_available})")

              if pre_sessions_created == "true" and pre_first_url:
                  lines.append(f"- Pre-existing sessions: {pre_session_count} created, first: [{pre_first_url}]({pre_first_url})")
              else:
                  lines.append(f"- Pre-existing sessions: not needed (actionable={pre_actionable})")

              lines.append("")
              lines.append(f"#### Comment Update")
              lines.append(f"- Method: `{'PATCH (update existing #' + existing_comment_id + ')' if existing_comment_id else 'POST (new comment)'}`")
              lines.append("")
              lines.append("</details>")
              lines.append("")

          # Footer
          lines.append("---")
          lines.append(f"*Automated by [Devin Security Review]({server_url}/{repo}/actions/runs/{run_id})*")
          lines.append("")

          # Hidden markers for attempt tracking (consumed by next run)
          attempt_pairs = [f"{k}={v}" for k, v in history["attempts"].items()]
          lines.append(f"<!-- attempts:{','.join(attempt_pairs)} -->")
          unfixable_keys = history.get("unfixable", [])
          lines.append(f"<!-- unfixable:{','.join(unfixable_keys)} -->")

          body = "\n".join(lines)
          with open("/tmp/comment_body.json", "w") as f:
              json.dump({"body": body}, f)

          # Also save the comment ID and method for the shell step
          with open("/tmp/comment_method.txt", "w") as f:
              if existing_comment_id:
                  f.write(f"PATCH\n{existing_comment_id}")
              else:
                  f.write("POST\n")

          print(f"Comment method: {'PATCH (update existing)' if existing_comment_id else 'POST (new comment)'}")
          print(f"Comment length: {len(body)} chars")
          COMMENT_EOF

          # Post or update the comment
          METHOD=$(head -1 /tmp/comment_method.txt)
          COMMENT_ID=$(tail -1 /tmp/comment_method.txt)

          if [ "$METHOD" = "PATCH" ] && [ -n "$COMMENT_ID" ]; then
            echo "Updating existing comment $COMMENT_ID..."
            curl -s -L \
              -X PATCH \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/comments/$COMMENT_ID" \
              -d @/tmp/comment_body.json | jq '{id: .id, html_url: .html_url}'
          else
            echo "Posting new comment..."
            curl -s -L \
              -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$PR_NUMBER/comments" \
              -d @/tmp/comment_body.json | jq '{id: .id, html_url: .html_url}'
          fi

          echo "PR comment posted/updated."

      # ------------------------------------------------------------------
      # STEP 9: Add label for unfixable alerts (developer visibility).
      # If any alerts are unfixable, add "devin:manual-review-needed" label
      # so developers can filter PRs that need human security review.
      # If all alerts are handled, remove the label.
      # ------------------------------------------------------------------
      - name: Manage PR labels for unfixable alerts
        if: steps.classify-alerts.outputs.total_count != '0'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          PR_NUMBER: ${{ steps.pr-context.outputs.pr_number }}
        run: |
          REPO="${{ github.repository }}"
          PR_NUMBER="$PR_NUMBER"
          UNFIXABLE_COUNT="${{ steps.filter-alerts.outputs.unfixable_count }}"
          LABEL="devin:manual-review-needed"

          if [ "$UNFIXABLE_COUNT" != "0" ] && [ -n "$UNFIXABLE_COUNT" ]; then
            echo "Adding label '$LABEL' to PR #$PR_NUMBER (unfixable alerts exist)..."
            # Create the label if it doesn't exist
            curl -s -L \
              -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/labels" \
              -d "{\"name\": \"$LABEL\", \"color\": \"d93f0b\", \"description\": \"Security alerts that Devin could not auto-fix — requires manual developer review\"}" \
              2>/dev/null || true

            # Add label to PR
            curl -s -L \
              -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$PR_NUMBER/labels" \
              -d "{\"labels\": [\"$LABEL\"]}" | jq '.[].name' 2>/dev/null || true
          else
            echo "No unfixable alerts. Removing label if present..."
            curl -s -L \
              -X DELETE \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$PR_NUMBER/labels/$LABEL" \
              2>/dev/null || true
          fi

      # ------------------------------------------------------------------
      # STEP 10: Post clean "no alerts" comment when all alerts are fixed.
      # If a previous run found alerts but now there are 0, update the
      # comment to show everything is resolved (safe-to-merge signal).
      # ------------------------------------------------------------------
      - name: Update comment when all alerts resolved
        if: steps.classify-alerts.outputs.total_count == '0'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          PR_NUMBER: ${{ steps.pr-context.outputs.pr_number }}
        run: |
          REPO="${{ github.repository }}"
          PR_NUMBER="$PR_NUMBER"

          # Find existing comment with pagination (enterprise PRs can have >100 comments)
          PAGE=1
          echo '[]' > /tmp/pr_comments_resolve.json
          while true; do
            curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$PR_NUMBER/comments?per_page=100&page=$PAGE" \
              > /tmp/pr_comments_resolve_page.json
            PAGE_COUNT=$(jq 'if type == "array" then length else 0 end' /tmp/pr_comments_resolve_page.json)
            if [ "$PAGE_COUNT" -eq 0 ]; then
              break
            fi
            jq -s '.[0] + .[1]' /tmp/pr_comments_resolve.json /tmp/pr_comments_resolve_page.json > /tmp/pr_comments_resolve_merged.json
            mv /tmp/pr_comments_resolve_merged.json /tmp/pr_comments_resolve.json
            if [ "$PAGE_COUNT" -lt 100 ]; then
              break
            fi
            PAGE=$((PAGE + 1))
          done

          COMMENT_ID=$(jq -r '[.[] | select(.body | contains("<!-- devin-security-review -->"))] | last | .id // empty' /tmp/pr_comments_resolve.json)

          if [ -n "$COMMENT_ID" ]; then
            echo "Updating existing comment to show all-clear..."
            BODY="<!-- devin-security-review -->\n## Devin Security Review\n\n**All CodeQL alerts have been resolved.**\n\nNo security issues remain on this PR. Safe to merge from a security standpoint.\n\n---\n*Automated by [Devin Security Review](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})*\n<!-- attempts: -->\n<!-- unfixable: -->"

            BODY_JSON=$(echo -e "$BODY" | python3 -c "import sys,json; print(json.dumps({'body': sys.stdin.read()}))")

            curl -s -L \
              -X PATCH \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/comments/$COMMENT_ID" \
              -d "$BODY_JSON" | jq '{id: .id, html_url: .html_url}'

            # Remove manual review label since everything is fixed
            curl -s -L \
              -X DELETE \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$PR_NUMBER/labels/devin:manual-review-needed" \
              2>/dev/null || true
          fi

      # ------------------------------------------------------------------
      # STEP 11: Summary for GitHub Actions UI.
      # ------------------------------------------------------------------
      - name: Fail workflow if session creation failed (honest exit code)
        if: always()
        run: |
          NEW_FAILED="${{ steps.devin-new-pr.outputs.session_failed }}"
          DEVIN_AVAILABLE="${{ steps.health-check.outputs.devin_available }}"
          NEW_ACTIONABLE="${{ steps.filter-alerts.outputs.new_actionable_count }}"
          PRE_ACTIONABLE="${{ steps.filter-alerts.outputs.pre_actionable_count }}"

          if [ "$NEW_FAILED" = "true" ]; then
            echo "::error::Devin session creation failed for new-in-PR alerts. Marking workflow as failed (EC11: honest exit code)."
            exit 1
          fi

          if [ "$DEVIN_AVAILABLE" = "false" ] && { [ "$NEW_ACTIONABLE" != "0" ] && [ -n "$NEW_ACTIONABLE" ]; }; then
            echo "::error::Devin API unavailable but actionable alerts exist. Marking workflow as failed (EC11: honest exit code)."
            exit 1
          fi

          echo "Session creation status: OK"

      - name: Summary
        if: always()
        run: |
          echo "## Devin Security Review Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total alerts**: ${{ steps.classify-alerts.outputs.total_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **New in PR**: ${{ steps.classify-alerts.outputs.new_in_pr_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Pre-existing**: ${{ steps.classify-alerts.outputs.preexisting_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Actionable (new)**: ${{ steps.filter-alerts.outputs.new_actionable_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Actionable (pre)**: ${{ steps.filter-alerts.outputs.pre_actionable_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Unfixable**: ${{ steps.filter-alerts.outputs.unfixable_count }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### New-in-PR Session" >> $GITHUB_STEP_SUMMARY
          echo "- **Created**: ${{ steps.devin-new-pr.outputs.session_created }}" >> $GITHUB_STEP_SUMMARY
          echo "- **URL**: ${{ steps.devin-new-pr.outputs.session_url }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Pre-existing Sessions" >> $GITHUB_STEP_SUMMARY
          echo "- **Created**: ${{ steps.devin-preexisting.outputs.sessions_created }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Count**: ${{ steps.devin-preexisting.outputs.session_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Fix branch**: ${{ steps.devin-preexisting.outputs.fix_branch }}" >> $GITHUB_STEP_SUMMARY
