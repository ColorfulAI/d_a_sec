# Devin Security Backlog Sweep — Orchestrator Workflow
#
# This workflow processes the ENTIRE backlog of open CodeQL alerts on main.
# It uses a sub-workflow fan-out pattern:
#   1. Fetches all open CodeQL alerts on main (paginated)
#   2. Reads cursor from tracking issue (skip already-processed alerts)
#   3. Groups remaining alerts into batches (15 per batch, grouped by file)
#   4. Dispatches child workflows (devin-security-batch.yml), one per batch
#   5. Polls child workflow statuses (rolling window of max 3 active)
#   6. As each child completes, updates cursor
#   7. Continues until all batches are dispatched and completed
#
# KEY DESIGN DECISIONS (see DESIGN.md for full rationale):
#
# 1. SUB-WORKFLOW FAN-OUT: Each batch is a separate workflow run with its own
#    logs, timeout, and failure isolation. No 6-hour timeout concern.
#
# 2. STREAMING PR CREATION: PRs are created by child workflows as they complete,
#    not waiting for all batches. First PR available in ~10 minutes.
#
# 3. SESSION RESERVATION: Self-limits to 3 concurrent children, reserving 2
#    slots for PR workflows that may fire at any time.
#
# 4. CURSOR-BASED STATELESS PICKUP: A GitHub issue comment stores processed
#    alert IDs so the next run skips already-handled alerts.
#
# 5. WAVE-BASED PROCESSING: The orchestrator fills available slots, polls for
#    completion, and immediately backfills freed slots with pending batches.

name: Devin Security Backlog

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      max_batches:
        description: "Maximum number of batches to process (0 = unlimited)"
        required: false
        type: string
        default: "0"
      dry_run:
        description: "Log what would be done without dispatching children"
        required: false
        type: boolean
        default: false
      max_concurrent:
        description: "Maximum concurrent child workflows (default: 3, max: 5)"
        required: false
        type: string
        default: "3"
      reset_cursor:
        description: "Reset cursor and reprocess all alerts"
        required: false
        type: boolean
        default: false
      dispatch_ref:
        description: "Branch ref for child workflow dispatch (default: current branch)"
        required: false
        type: string
        default: ""

permissions:
  contents: write
  security-events: read
  pull-requests: write
  issues: write
  actions: write

concurrency:
  group: devin-security-backlog
  cancel-in-progress: false

jobs:
  orchestrate:
    runs-on: ubuntu-latest
    env:
      MAX_CONCURRENT: ${{ github.event.inputs.max_concurrent || '3' }}
      MAX_BATCHES: ${{ github.event.inputs.max_batches || '0' }}
      DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
      RESET_CURSOR: ${{ github.event.inputs.reset_cursor || 'false' }}
      ALERTS_PER_BATCH: 15
      POLL_INTERVAL: 60
      MAX_CHILD_RUNTIME: 3600
      SAFETY_TIMEOUT: 19800
      DISPATCH_REF: ${{ github.event.inputs.dispatch_ref || github.ref_name }}

    steps:
      - uses: actions/checkout@v4

      # ----------------------------------------------------------------
      # STEP 1: Health check
      # ----------------------------------------------------------------
      - name: Health check
        id: health-check
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
        run: |
          HEALTHY=true

          if [ -z "$GH_PAT" ]; then
            echo "::error::GH_PAT secret is not set."
            HEALTHY=false
          fi

          if [ -z "$DEVIN_API_KEY" ]; then
            echo "::error::DEVIN_API_KEY secret is not set."
            HEALTHY=false
          else
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
              -X GET "https://api.devin.ai/v1/sessions" \
              -H "Authorization: Bearer $DEVIN_API_KEY")
            if [ "$HTTP_CODE" = "401" ] || [ "$HTTP_CODE" = "403" ]; then
              echo "::error::DEVIN_API_KEY returned HTTP $HTTP_CODE"
              HEALTHY=false
            else
              echo "Devin API: reachable (HTTP $HTTP_CODE)"
            fi
          fi

          if [ "$HEALTHY" = "false" ]; then
            exit 1
          fi
          echo "Health check passed."

      # ----------------------------------------------------------------
      # STEP 2: Fetch all open CodeQL alerts on main
      # ----------------------------------------------------------------
      - name: Fetch all open CodeQL alerts
        id: fetch-alerts
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          REPO="${{ github.repository }}"
          echo "Fetching all open CodeQL alerts on main..."

          PAGE=1
          echo '[]' > /tmp/all_alerts.json
          TOTAL_FETCHED=0

          while true; do
            RESP=$(curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/code-scanning/alerts?ref=refs/heads/main&state=open&per_page=100&page=$PAGE")

            PAGE_COUNT=$(echo "$RESP" | jq 'if type == "array" then length else 0 end')
            if [ "$PAGE_COUNT" -eq 0 ] || [ "$PAGE_COUNT" = "null" ]; then
              break
            fi

            echo "$RESP" > /tmp/alerts_page.json
            jq -s '.[0] + .[1]' /tmp/all_alerts.json /tmp/alerts_page.json > /tmp/alerts_merged.json
            mv /tmp/alerts_merged.json /tmp/all_alerts.json
            TOTAL_FETCHED=$((TOTAL_FETCHED + PAGE_COUNT))

            echo "Page $PAGE: $PAGE_COUNT alerts (total: $TOTAL_FETCHED)"

            if [ "$PAGE_COUNT" -lt 100 ]; then
              break
            fi
            PAGE=$((PAGE + 1))
          done

          echo "Total open alerts on main: $TOTAL_FETCHED"
          echo "total_alerts=$TOTAL_FETCHED" >> $GITHUB_OUTPUT

          if [ "$TOTAL_FETCHED" -eq 0 ]; then
            echo "No open alerts to process."
            echo "skip=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "skip=false" >> $GITHUB_OUTPUT

      # ----------------------------------------------------------------
      # STEP 3: Find or create tracking issue for cursor state
      # ----------------------------------------------------------------
      - name: Find or create tracking issue
        id: tracking-issue
        if: steps.fetch-alerts.outputs.skip != 'true'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          REPO="${{ github.repository }}"
          LABEL="devin:backlog-tracker"

          # Create label if it doesn't exist
          LABEL_HTTP=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/labels/$(echo "$LABEL" | sed 's/:/%3A/g')")
          if [ "$LABEL_HTTP" = "404" ]; then
            curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/labels" \
              -d "{\"name\": \"$LABEL\", \"color\": \"0075ca\", \"description\": \"Tracks Devin security backlog sweep progress\"}" \
              2>/dev/null || true
          fi

          # Find existing tracking issue
          ISSUES=$(curl -s -L \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/issues?labels=$LABEL&state=open&per_page=1")
          ISSUE_NUM=$(echo "$ISSUES" | jq -r '.[0].number // empty')

          if [ -z "$ISSUE_NUM" ]; then
            echo "Creating tracking issue..."
            RESP=$(curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues" \
              -d "{
                \"title\": \"Devin Security Backlog — Progress Tracker\",
                \"body\": \"This issue tracks the progress of the Devin Security Backlog Sweep.\\n\\nDo not edit this issue manually — it is updated automatically by the workflow.\",
                \"labels\": [\"$LABEL\"]
              }")
            ISSUE_NUM=$(echo "$RESP" | jq -r '.number')
            echo "Created tracking issue #$ISSUE_NUM"
          else
            echo "Found tracking issue #$ISSUE_NUM"
          fi

          echo "issue_number=$ISSUE_NUM" >> $GITHUB_OUTPUT

      # ----------------------------------------------------------------
      # STEP 4: Read cursor from tracking issue
      # ----------------------------------------------------------------
      - name: Read cursor
        id: read-cursor
        if: steps.fetch-alerts.outputs.skip != 'true'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          ISSUE_NUM: ${{ steps.tracking-issue.outputs.issue_number }}
        run: |
          REPO="${{ github.repository }}"

          if [ "$RESET_CURSOR" = "true" ]; then
            echo "Cursor reset requested. Starting fresh."
            echo '{"processed_alert_ids":[],"unfixable_alert_ids":[],"attempted_alert_ids":[],"in_progress":{},"total_fixed":0,"total_unfixable":0,"total_attempted":0}' > /tmp/cursor.json
            echo "cursor_found=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Fetch comments and find cursor
          COMMENTS=$(curl -s -L \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/issues/$ISSUE_NUM/comments?per_page=100")

          # Save comments for Python to read
          echo "$COMMENTS" > /tmp/cursor_comments.json

          python3 << 'CURSOR_PARSE_EOF'
          import json, os

          try:
              with open("/tmp/cursor_comments.json") as f:
                  comments = json.load(f)
                  if not isinstance(comments, list):
                      comments = []
          except:
              comments = []

          cursor = None
          comment_id = ""
          marker = "<!-- backlog-cursor -->"

          for c in comments:
              body = c.get("body", "")
              if marker in body:
                  comment_id = str(c.get("id", ""))
                  start = body.find("```json\n")
                  end = body.find("\n```", start + 8) if start >= 0 else -1
                  if start >= 0 and end >= 0:
                      json_str = body[start + 8:end]
                      try:
                          cursor = json.loads(json_str)
                      except:
                          print("WARNING: Cursor JSON corrupted. Starting fresh.")

          if cursor is None:
              cursor = {
                  "processed_alert_ids": [],
                  "unfixable_alert_ids": [],
                  "attempted_alert_ids": [],
                  "in_progress": {},
                  "total_fixed": 0,
                  "total_unfixable": 0,
                  "total_attempted": 0
              }
              print("No cursor found. Starting fresh scan.")
          else:
              print(f"Cursor loaded: {len(cursor.get('processed_alert_ids', []))} processed, {len(cursor.get('unfixable_alert_ids', []))} unfixable, {len(cursor.get('attempted_alert_ids', []))} attempted")

          with open("/tmp/cursor.json", "w") as f:
              json.dump(cursor, f, indent=2)

          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"cursor_comment_id={comment_id}\n")
              f.write(f"cursor_found={'true' if comment_id else 'false'}\n")
          CURSOR_PARSE_EOF

      # ----------------------------------------------------------------
      # STEP 5: Filter alerts and group into batches
      # ----------------------------------------------------------------
      - name: Filter and batch alerts
        id: batch-alerts
        if: steps.fetch-alerts.outputs.skip != 'true'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          python3 << 'BATCH_EOF'
          import json, os, time

          alerts_per_batch = int(os.environ.get("ALERTS_PER_BATCH", "15"))
          max_batches_str = os.environ.get("MAX_BATCHES", "0")
          max_batches = int(max_batches_str) if max_batches_str else 0

          with open("/tmp/all_alerts.json") as f:
              all_alerts = json.load(f)

          with open("/tmp/cursor.json") as f:
              cursor = json.load(f)

          processed = set(cursor.get("processed_alert_ids", []))
          unfixable = set(cursor.get("unfixable_alert_ids", []))
          attempted = set(cursor.get("attempted_alert_ids", []))

          # Bug #18b fix: Re-verify unfixable alerts before skipping them.
          # After a fix PR is merged and CodeQL re-runs, previously "unfixable"
          # alerts may now be fixed. Re-query CodeQL to check.
          import urllib.request, urllib.error
          HTTP_TIMEOUT = 30
          gh_pat = os.environ.get("GH_PAT", "")
          repo_name = os.environ.get("GITHUB_REPOSITORY", "")
          re_headers = {
              "Authorization": f"token {gh_pat}",
              "Accept": "application/vnd.github+json"
          }

          reverified_fixed = []
          if unfixable:
              print(f"\nRe-verifying {len(unfixable)} unfixable alerts...")
              for uid in list(unfixable):
                  try:
                      url = f"https://api.github.com/repos/{repo_name}/code-scanning/alerts/{uid}"
                      req = urllib.request.Request(url, headers=re_headers)
                      with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
                          alert_data = json.loads(resp.read())
                      state = alert_data.get("state", "open")
                      if state in ("fixed", "dismissed"):
                          reverified_fixed.append(uid)
                          print(f"  Alert #{uid}: now {state} on main -> moving to processed")
                      else:
                          print(f"  Alert #{uid}: still {state} -> remains unfixable")
                  except Exception as e:
                      print(f"  Alert #{uid}: re-verify error ({e}) -> remains unfixable")
                  time.sleep(0.3)

          # Also re-verify attempted alerts (fix PRs may have been merged)
          reverified_attempted = []
          if attempted:
              print(f"\nRe-verifying {len(attempted)} attempted alerts...")
              for uid in list(attempted):
                  try:
                      url = f"https://api.github.com/repos/{repo_name}/code-scanning/alerts/{uid}"
                      req = urllib.request.Request(url, headers=re_headers)
                      with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
                          alert_data = json.loads(resp.read())
                      state = alert_data.get("state", "open")
                      if state in ("fixed", "dismissed"):
                          reverified_attempted.append(uid)
                          print(f"  Alert #{uid}: now {state} on main -> moving to processed")
                      else:
                          print(f"  Alert #{uid}: still {state} -> remains attempted")
                  except Exception as e:
                      print(f"  Alert #{uid}: re-verify error ({e}) -> remains attempted")
                  time.sleep(0.3)

          # Apply re-verification results to cursor
          for uid in reverified_fixed:
              unfixable.discard(uid)
              processed.add(uid)
              if uid not in cursor["processed_alert_ids"]:
                  cursor["processed_alert_ids"].append(uid)
              if uid in cursor.get("unfixable_alert_ids", []):
                  cursor["unfixable_alert_ids"].remove(uid)
          for uid in reverified_attempted:
              attempted.discard(uid)
              processed.add(uid)
              if uid not in cursor["processed_alert_ids"]:
                  cursor["processed_alert_ids"].append(uid)
              if uid in cursor.get("attempted_alert_ids", []):
                  cursor["attempted_alert_ids"].remove(uid)

          if reverified_fixed or reverified_attempted:
              print(f"\nRe-verification results: {len(reverified_fixed)} unfixable->fixed, {len(reverified_attempted)} attempted->fixed")
              cursor["total_unfixable"] = len(cursor.get("unfixable_alert_ids", []))
              cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))
              # Save updated cursor so orchestrator step sees re-verified state
              with open("/tmp/cursor.json", "w") as f:
                  json.dump(cursor, f, indent=2)

          # Filter out already-processed, unfixable, and attempted alerts
          remaining = []
          skipped_processed = 0
          skipped_unfixable = 0
          skipped_attempted = 0

          for a in all_alerts:
              alert_num = a.get("number")
              if alert_num in processed:
                  skipped_processed += 1
                  continue
              if alert_num in unfixable:
                  skipped_unfixable += 1
                  continue
              if alert_num in attempted:
                  skipped_attempted += 1
                  continue
              remaining.append(a)

          print(f"\nTotal open alerts: {len(all_alerts)}")
          print(f"Already processed (confirmed fixed): {skipped_processed}")
          print(f"Unfixable (skipped, needs human review): {skipped_unfixable}")
          print(f"Attempted (skipped, pending PR merge): {skipped_attempted}")
          print(f"Remaining to process: {len(remaining)}")

          if not remaining:
              print("No new alerts to process.")
              gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
              with open(gh_out, "a") as f:
                  f.write("batch_count=0\n")
                  f.write("total_remaining=0\n")
              with open("/tmp/batches.json", "w") as f:
                  json.dump([], f)
              exit(0)

          # Group by file for efficient Devin sessions
          severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
          file_groups = {}
          for a in remaining:
              loc = a.get("most_recent_instance", {}).get("location", {})
              path = loc.get("path", "unknown")
              file_groups.setdefault(path, []).append(a)

          # Sort each file group by severity
          for fg in file_groups.values():
              fg.sort(key=lambda a: severity_order.get(
                  a.get("rule", {}).get("security_severity_level", "low"), 3))

          # Sort files by most severe alert
          sorted_files = sorted(
              file_groups.items(),
              key=lambda kv: min(severity_order.get(
                  a.get("rule", {}).get("security_severity_level", "low"), 3) for a in kv[1])
          )

          # Build batches: ALL alerts per file stay together (never split a file)
          #
          # WHY: If a batch touches file X but only fixes 3 of 10 alerts in X,
          # the PR modifies X and CodeQL reports the remaining 7 as failures.
          # By keeping all alerts for a file in the same batch, the PR either
          # cleans the file completely or doesn't touch it at all.
          #
          # STRATEGY: Pack whole-file groups into batches using first-fit.
          # A batch can exceed alerts_per_batch if a single file has more alerts
          # than the cap (rare but possible). We never break a file across batches.
          batches = []
          file_remaining = {}
          for fp, fa in sorted_files:
              file_remaining[fp] = list(fa)

          current_batch = []
          current_batch_files = set()
          for fp, _ in sorted_files:
              if not file_remaining.get(fp):
                  continue
              file_alerts = file_remaining[fp]
              if not file_alerts:
                  continue

              # If adding this file would exceed the cap AND we already have alerts,
              # close the current batch and start a new one
              if current_batch and len(current_batch) + len(file_alerts) > alerts_per_batch:
                  batch_id = len(batches) + 1
                  alert_ids = [a.get("number") for a in current_batch]
                  ts = int(time.time())
                  batches.append({
                      "batch_id": batch_id,
                      "alert_ids": alert_ids,
                      "alert_count": len(alert_ids),
                      "files": list(current_batch_files),
                      "branch_name": f"devin/security-batch-{batch_id}-{ts}"
                  })
                  current_batch = []
                  current_batch_files = set()

              # Add ALL alerts for this file (never split)
              current_batch.extend(file_alerts)
              current_batch_files.add(fp)
              file_remaining[fp] = []

          # Flush remaining batch
          if current_batch:
              batch_id = len(batches) + 1
              alert_ids = [a.get("number") for a in current_batch]
              ts = int(time.time())
              batches.append({
                  "batch_id": batch_id,
                  "alert_ids": alert_ids,
                  "alert_count": len(alert_ids),
                  "files": list(current_batch_files),
                  "branch_name": f"devin/security-batch-{batch_id}-{ts}"
              })

          # Apply max_batches limit
          if max_batches > 0:
              batches = batches[:max_batches]
              print(f"Limited to {max_batches} batches (per input)")

          print(f"\nCreated {len(batches)} batches:")
          for b in batches:
              print(f"  Batch {b['batch_id']}: {b['alert_count']} alerts in {', '.join(b['files'][:3])}")

          with open("/tmp/batches.json", "w") as f:
              json.dump(batches, f, indent=2)

          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"batch_count={len(batches)}\n")
              f.write(f"total_remaining={len(remaining)}\n")
          BATCH_EOF

      # ----------------------------------------------------------------
      # STEP 6: Dispatch child workflows and manage rolling window
      # This is the core orchestrator loop. It:
      #   - Dispatches up to MAX_CONCURRENT children
      #   - Polls for completion every POLL_INTERVAL seconds
      #   - Backfills freed slots with pending batches
      #   - Updates cursor as batches complete
      #   - Exits when all batches are dispatched + completed
      # ----------------------------------------------------------------
      - name: Dispatch and manage child workflows
        id: orchestrate
        if: steps.batch-alerts.outputs.batch_count != '0'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
          ISSUE_NUM: ${{ steps.tracking-issue.outputs.issue_number }}
          CURSOR_COMMENT_ID: ${{ steps.read-cursor.outputs.cursor_comment_id }}
          PYTHONUNBUFFERED: "1"
        run: |
          python3 << 'ORCHESTRATE_EOF'
          import json, os, sys, time, urllib.request, urllib.error

          # Force unbuffered stdout so logs appear in real-time (Bug #11 fix)
          sys.stdout.reconfigure(line_buffering=True)

          HTTP_TIMEOUT = 30  # seconds — prevent hanging on slow API responses (Bug #12 fix)

          repo = os.environ.get("GITHUB_REPOSITORY", "")
          gh_pat = os.environ["GH_PAT"]
          devin_key = os.environ["DEVIN_API_KEY"]
          issue_num = os.environ.get("ISSUE_NUM", "")
          cursor_comment_id = os.environ.get("CURSOR_COMMENT_ID", "")
          dry_run = os.environ.get("DRY_RUN", "false") == "true"
          max_concurrent = min(int(os.environ.get("MAX_CONCURRENT", "3")), 5)
          poll_interval = int(os.environ.get("POLL_INTERVAL", "60"))
          max_child_runtime = int(os.environ.get("MAX_CHILD_RUNTIME", "3600"))
          safety_timeout = int(os.environ.get("SAFETY_TIMEOUT", "19800"))
          run_id = os.environ.get("GITHUB_RUN_ID", "")

          with open("/tmp/batches.json") as f:
              all_batches = json.load(f)
          with open("/tmp/cursor.json") as f:
              cursor = json.load(f)

          if not all_batches:
              print("No batches to process.")
              exit(0)

          print(f"Orchestrator config:")
          print(f"  Batches: {len(all_batches)}")
          print(f"  Max concurrent: {max_concurrent}")
          print(f"  Poll interval: {poll_interval}s")
          print(f"  Max child runtime: {max_child_runtime}s ({max_child_runtime//60} min)")
          print(f"  Safety timeout: {safety_timeout}s ({safety_timeout//60} min)")
          print(f"  Dry run: {dry_run}")

          def gh_api(method, url, data=None):
              req = urllib.request.Request(
                  url,
                  data=json.dumps(data).encode() if data else None,
                  headers={
                      "Authorization": f"token {gh_pat}",
                      "Accept": "application/vnd.github+json",
                      "Content-Type": "application/json"
                  },
                  method=method
              )
              try:
                  resp = urllib.request.urlopen(req, timeout=HTTP_TIMEOUT)
                  body = resp.read()
                  status_code = resp.status
                  if not body or status_code == 204:
                      return {}, status_code
                  return json.loads(body), status_code
              except urllib.error.HTTPError as e:
                  body = e.read().decode()[:500]
                  return {"error": body, "status": e.code}, e.code
              except Exception as e:
                  return {"error": str(e)}, 0

          def check_active_devin_sessions():
              try:
                  req = urllib.request.Request(
                      "https://api.devin.ai/v1/sessions?status_in=running,started",
                      headers={"Authorization": f"Bearer {devin_key}"}
                  )
                  resp = urllib.request.urlopen(req, timeout=HTTP_TIMEOUT)
                  data = json.loads(resp.read())
                  sessions = data.get("sessions", [])
                  count = len(sessions)
                  print(f"  Active Devin sessions: {count}")
                  return count
              except Exception as e:
                  print(f"  Warning: Could not check active sessions: {e}")
                  return 0

          # Shared headers for direct urllib requests (artifact download, etc.)
          # gh_api() has its own internal headers, but artifact download uses urllib directly.
          headers = {
              "Authorization": f"token {gh_pat}",
              "Accept": "application/vnd.github+json"
          }

          # Bug #20/#21 fix: GitHub artifact download URLs return 302 redirects
          # to Azure Blob Storage. urllib forwards the Authorization header to
          # Azure, causing 403 (bug #20). Custom redirect handler caused 400
          # (bug #21). Solution: use curl which natively strips auth headers
          # on cross-domain redirects (since curl 7.58+).
          def download_artifact_zip(url, gh_token):
              """Download artifact zip using curl (handles auth-stripping on redirect)."""
              import subprocess, tempfile
              tmp_path = tempfile.mktemp(suffix=".zip")
              result = subprocess.run(
                  ["curl", "-sL",
                   "-H", f"Authorization: token {gh_token}",
                   "-H", "Accept: application/vnd.github+json",
                   "-o", tmp_path,
                   "-w", "%{http_code}",
                   url],
                  capture_output=True, text=True, timeout=60
              )
              http_code = result.stdout.strip()
              if http_code == "200" and os.path.exists(tmp_path):
                  with open(tmp_path, "rb") as f:
                      data = f.read()
                  os.unlink(tmp_path)
                  return data
              else:
                  if os.path.exists(tmp_path):
                      os.unlink(tmp_path)
                  raise Exception(f"curl returned HTTP {http_code}: {result.stderr[:200]}")

          dispatch_ref = os.environ.get("DISPATCH_REF", "main")

          def dispatch_child(batch):
              alert_ids_str = ",".join(str(a) for a in batch["alert_ids"])
              url = f"https://api.github.com/repos/{repo}/actions/workflows/devin-security-batch.yml/dispatches"
              data = {
                  "ref": dispatch_ref,
                  "inputs": {
                      "batch_id": str(batch["batch_id"]),
                      "alert_ids": alert_ids_str,
                      "branch_name": batch["branch_name"],
                      "tracking_issue": str(issue_num)
                  }
              }
              result, status = gh_api("POST", url, data)
              return status == 204

          def check_child_status(run_id_val):
              url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id_val}"
              result, status = gh_api("GET", url)
              if status != 200:
                  return "unknown"
              return result.get("status", "unknown"), result.get("conclusion")

          def update_cursor(cursor_data):
              global cursor_comment_id
              marker = "<!-- backlog-cursor -->"
              cursor_json = json.dumps(cursor_data, indent=2)
              body = f"{marker}\n## Backlog Sweep Cursor\n\n*Last updated: {time.strftime('%Y-%m-%d %H:%M UTC', time.gmtime())}*\n\n```json\n{cursor_json}\n```\n\n---\n*Updated by [orchestrator run](https://github.com/{repo}/actions/runs/{run_id})*"
              url_base = f"https://api.github.com/repos/{repo}/issues"

              if cursor_comment_id:
                  result, status = gh_api("PATCH", f"{url_base}/comments/{cursor_comment_id}", {"body": body})
                  if status == 200:
                      return True
              # Create new comment (Bug #26 fix: save the new comment ID so subsequent
              # calls PATCH instead of creating more comments)
              result, status = gh_api("POST", f"{url_base}/{issue_num}/comments", {"body": body})
              if status == 201 and result.get("id"):
                  new_id = str(result["id"])
                  # Bug #32 fix: Delete stale cursor comments from previous runs.
                  # This keeps the tracking issue clean with exactly one active cursor comment.
                  try:
                      comments_result, c_status = gh_api("GET", f"{url_base}/{issue_num}/comments?per_page=100")
                      if c_status == 200 and isinstance(comments_result, list):
                          for c in comments_result:
                              c_id = str(c.get("id", ""))
                              if c_id != new_id and marker in c.get("body", ""):
                                  _, del_status = gh_api("DELETE", f"{url_base}/comments/{c_id}")
                                  if del_status == 204:
                                      print(f"    Deleted stale cursor comment {c_id}")
                  except Exception as e:
                      print(f"    Warning: stale cursor cleanup failed ({e}), continuing")
                  cursor_comment_id = new_id
                  print(f"    Created cursor comment {cursor_comment_id}")
              return status == 201

          # ============================================================
          # MAIN ORCHESTRATOR LOOP
          # ============================================================
          pending_batches = list(all_batches)
          active_children = {}  # run_id -> {batch, dispatched_at, batch_id}
          completed_batches = []
          failed_batches = []
          start_time = time.time()

          if dry_run:
              print("\n=== DRY RUN MODE ===")
              for b in pending_batches:
                  print(f"  Would dispatch: Batch {b['batch_id']} ({b['alert_count']} alerts) -> {b['branch_name']}")
              print(f"\nTotal: {len(pending_batches)} batches, {sum(b['alert_count'] for b in pending_batches)} alerts")
              exit(0)

          print(f"\n{'='*60}")
          print(f"Starting orchestrator loop")
          print(f"{'='*60}\n")

          # Initial fill: dispatch up to max_concurrent children
          # NOTE (Bug #14 fix): We no longer gate on check_active_devin_sessions().
          # The Devin API session count includes sessions from ALL workflows on the
          # account (PR reviews, other repos, etc.), which blocks dispatch even when
          # we have capacity. Instead, we control concurrency by tracking our own
          # active children. Child workflows handle Devin API 429 rate limits internally.
          while len(active_children) < max_concurrent and pending_batches:
              batch = pending_batches.pop(0)
              dispatch_time = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

              print(f"Dispatching Batch {batch['batch_id']} ({batch['alert_count']} alerts)...")
              success = dispatch_child(batch)

              if success:
                  # We need to find the workflow run ID. Wait a moment for GitHub to register it.
                  time.sleep(5)
                  # Store with dispatch timestamp so we can find it
                  active_children[f"batch_{batch['batch_id']}"] = {
                      "batch": batch,
                      "dispatched_at": time.time(),
                      "dispatch_time_str": dispatch_time,
                      "run_id": None,  # Will be resolved during polling
                      "status": "dispatched"
                  }
                  print(f"  Dispatched successfully")
              else:
                  print(f"  Dispatch FAILED — will retry later")
                  pending_batches.insert(0, batch)
                  time.sleep(10)

          # Poll loop
          poll_count = 0
          while active_children or pending_batches:
              # Safety timeout check
              elapsed = time.time() - start_time
              if elapsed > safety_timeout:
                  print(f"\n  SAFETY TIMEOUT ({safety_timeout//60} min). Saving cursor and exiting.")
                  print(f"  Completed: {len(completed_batches)}, Failed: {len(failed_batches)}, Remaining: {len(pending_batches) + len(active_children)}")
                  break

              time.sleep(poll_interval)
              poll_count += 1
              print(f"\n--- Poll #{poll_count} (elapsed: {int(elapsed)}s, active: {len(active_children)}, pending: {len(pending_batches)}) ---")

              # Re-dispatch pending batches if child slots are free (Bug #13 + #14 fix)
              if pending_batches and len(active_children) < max_concurrent:
                  batch = pending_batches.pop(0)
                  dispatch_time = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
                  print(f"  Re-dispatch: Batch {batch['batch_id']} ({batch['alert_count']} alerts)...")
                  if dispatch_child(batch):
                      time.sleep(5)
                      active_children[f"batch_{batch['batch_id']}"] = {
                          "batch": batch,
                          "dispatched_at": time.time(),
                          "dispatch_time_str": dispatch_time,
                          "run_id": None,
                          "status": "dispatched"
                      }
                      print(f"  Re-dispatched successfully")
                  else:
                      print(f"  Re-dispatch failed — will retry next poll")
                      pending_batches.insert(0, batch)

              # Resolve run IDs for newly dispatched children
              # NOTE (Bug #22 fix): workflow_dispatch runs are returned newest-first
              # by the API. When multiple batches are dispatched close together,
              # iterating newest-first causes cross-matching (batch 1 grabs batch 2's
              # run). Fix: sort runs oldest-first so dispatch order matches run order.
              # Also resolve ALL unmatched children in one pass against the same
              # sorted run list to ensure correct 1:1 matching.
              unresolved = [(k, c) for k, c in active_children.items() if c["run_id"] is None]
              if unresolved:
                  # Use the earliest dispatch date for the query
                  earliest_date = min(c["dispatch_time_str"][:10] for _, c in unresolved)
                  url = f"https://api.github.com/repos/{repo}/actions/workflows/devin-security-batch.yml/runs?per_page=30&event=workflow_dispatch&created=>={earliest_date}"
                  result, status = gh_api("GET", url)
                  if status == 200:
                      already_matched = set(str(c.get("run_id")) for c in active_children.values() if c.get("run_id"))
                      # Sort oldest-first to match dispatch order
                      sorted_runs = sorted(result.get("workflow_runs", []), key=lambda r: r.get("created_at", ""))
                      # Sort unresolved children by dispatch time (oldest first)
                      unresolved_sorted = sorted(unresolved, key=lambda x: x[1]["dispatch_time_str"])
                      for key, child in unresolved_sorted:
                          for run in sorted_runs:
                              run_created = run.get("created_at", "")
                              run_id_val = run["id"]
                              if run_created >= child["dispatch_time_str"] and str(run_id_val) not in already_matched:
                                  child["run_id"] = run_id_val
                                  already_matched.add(str(run_id_val))
                                  print(f"  Resolved Batch {child['batch']['batch_id']} -> run {run_id_val} (timestamp match: created={run_created} >= dispatched={child['dispatch_time_str']})")
                                  break

              # Check status of active children
              for key, child in list(active_children.items()):
                  if child["run_id"] is None:
                      # Check for stuck dispatch
                      age = time.time() - child["dispatched_at"]
                      if age > 300:  # 5 minutes without finding run
                          print(f"  Batch {child['batch']['batch_id']}: No workflow run found after {int(age)}s — re-dispatching")
                          pending_batches.append(child["batch"])
                          del active_children[key]
                      continue

                  run_id_val = child["run_id"]
                  status_tuple = check_child_status(run_id_val)

                  if isinstance(status_tuple, tuple):
                      run_status, conclusion = status_tuple
                  else:
                      run_status, conclusion = status_tuple, None

                  child["status"] = run_status

                  if run_status == "completed":
                      batch = child["batch"]
                      if conclusion == "success":
                          print(f"  Batch {batch['batch_id']}: COMPLETED (success)")
                          completed_batches.append(batch)

                          # Fetch batch result artifact to get per-alert breakdown
                          # Bug #18 fix: Now includes attempted_alert_ids (file modified but not yet merged)
                          batch_unfixable = []
                          batch_fixed = []
                          batch_attempted = []
                          batch_pr_url = ""
                          artifact_fetched = False
                          try:
                              run_id_val = child["run_id"]
                              artifacts_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id_val}/artifacts"
                              req = urllib.request.Request(artifacts_url, headers=headers)
                              with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
                                  artifacts = json.loads(resp.read())
                              for artifact in artifacts.get("artifacts", []):
                                  if artifact["name"].startswith(f"batch-{batch['batch_id']}-"):
                                      print(f"    Found result artifact: {artifact['name']}")
                                      dl_url = artifact["archive_download_url"]
                                      import zipfile, io
                                      zip_data = download_artifact_zip(dl_url, gh_pat)
                                      z = zipfile.ZipFile(io.BytesIO(zip_data))
                                      for name in z.namelist():
                                          if name.endswith(".json"):
                                              result_data = json.loads(z.read(name))
                                              batch_fixed = [int(x) for x in result_data.get("fixed_alert_ids", [])]
                                              batch_attempted = [int(x) for x in result_data.get("attempted_alert_ids", [])]
                                              batch_unfixable = [int(x) for x in result_data.get("unfixable_alert_ids", [])]
                                              batch_pr_url = result_data.get("pr_url", "")
                                              artifact_fetched = True
                                              print(f"    Fixed: {len(batch_fixed)}, Attempted: {len(batch_attempted)}, Unfixable: {len(batch_unfixable)}, PR: {batch_pr_url or 'none'}")
                                      z.close()
                          except Exception as e:
                              print(f"    Could not fetch batch result artifact: {e}")
                              print(f"    Falling back: marking all {len(batch['alert_ids'])} alerts as attempted (not processed — status unknown)")

                          # Update cursor with three-state classification (Bug #18 fix)
                          if artifact_fetched and (batch_fixed or batch_attempted or batch_unfixable):
                              for aid in batch_fixed:
                                  if aid not in cursor["processed_alert_ids"]:
                                      cursor["processed_alert_ids"].append(aid)
                              for aid in batch_attempted:
                                  if aid not in cursor.get("attempted_alert_ids", []):
                                      cursor.setdefault("attempted_alert_ids", []).append(aid)
                              for aid in batch_unfixable:
                                  if aid not in cursor.get("unfixable_alert_ids", []):
                                      cursor.setdefault("unfixable_alert_ids", []).append(aid)
                              cursor["total_fixed"] = cursor.get("total_fixed", 0) + len(batch_fixed)
                              cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))
                              cursor["total_unfixable"] = len(cursor.get("unfixable_alert_ids", []))
                          else:
                              # Bug #24 fix: mark as attempted (not processed) when artifact is missing.
                              # "processed" implies confirmed fixed. Without artifact data, we don't know
                              # if the fix worked. Mark as attempted so the next run re-verifies.
                              for aid in batch["alert_ids"]:
                                  if aid not in cursor.get("attempted_alert_ids", []):
                                      cursor.setdefault("attempted_alert_ids", []).append(aid)
                              cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))
                          cursor["total_processed"] = len(cursor["processed_alert_ids"])

                          # Track batch PR URL for summary reporting (Bug #25 fix)
                          if batch_pr_url:
                              batch["pr_url"] = batch_pr_url
                      else:
                          print(f"  Batch {batch['batch_id']}: COMPLETED (conclusion={conclusion})")
                          failed_batches.append({**batch, "conclusion": conclusion})
                          # Bug #27 fix: Track failed batch alerts as "attempted" in cursor.
                          # Without this, the next orchestrator run treats them as unprocessed
                          # and re-dispatches them. Mark as attempted so re-verification can
                          # check if a partial fix landed, or a future run can detect them.
                          for aid in batch["alert_ids"]:
                              if aid not in cursor.get("attempted_alert_ids", []):
                                  cursor.setdefault("attempted_alert_ids", []).append(aid)
                          cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))

                      del active_children[key]

                      # Backfill: dispatch next pending batch
                      # NOTE (Bug #14/#17 fix): Use child-count concurrency, not Devin session count.
                      # check_active_devin_sessions() includes ALL sessions on the account,
                      # which blocks backfill when external sessions are present.
                      if pending_batches and len(active_children) < max_concurrent:
                              next_batch = pending_batches.pop(0)
                              dispatch_time = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
                              print(f"  Backfill: Dispatching Batch {next_batch['batch_id']}...")
                              if dispatch_child(next_batch):
                                  time.sleep(5)
                                  active_children[f"batch_{next_batch['batch_id']}"] = {
                                      "batch": next_batch,
                                      "dispatched_at": time.time(),
                                      "dispatch_time_str": dispatch_time,
                                      "run_id": None,
                                      "status": "dispatched"
                                  }

                      # Update cursor after each completion (streaming)
                      update_cursor(cursor)

                  elif run_status in ("queued", "in_progress", "waiting"):
                      age = time.time() - child["dispatched_at"]
                      if age > max_child_runtime:
                          print(f"  Batch {child['batch']['batch_id']}: STALE ({int(age)}s > {max_child_runtime}s) — evicting")
                          failed_batches.append({**child["batch"], "conclusion": "timeout_evicted"})
                          del active_children[key]
                      else:
                          print(f"  Batch {child['batch']['batch_id']}: {run_status} ({int(age)}s)")

                  else:
                      # Bug #22 fix: handle unexpected status (e.g. "unknown" from API failure)
                      age = time.time() - child["dispatched_at"]
                      print(f"  Batch {child['batch']['batch_id']}: unexpected status '{run_status}' ({int(age)}s)")
                      if age > max_child_runtime:
                          print(f"  Batch {child['batch']['batch_id']}: evicting after {int(age)}s with status '{run_status}'")
                          failed_batches.append({**child["batch"], "conclusion": f"unknown_status_{run_status}"})
                          del active_children[key]

          # ============================================================
          # FINAL CURSOR UPDATE
          # ============================================================
          cursor["last_run"] = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
          cursor["last_run_id"] = run_id
          update_cursor(cursor)

          # Summary
          total_time = int(time.time() - start_time)
          unfixable_ids = cursor.get("unfixable_alert_ids", [])
          attempted_ids = cursor.get("attempted_alert_ids", [])
          print(f"\n{'='*60}")
          print(f"Orchestrator complete")
          print(f"{'='*60}")
          print(f"  Total time: {total_time}s ({total_time//60} min)")
          print(f"  Batches completed: {len(completed_batches)}")
          print(f"  Batches failed: {len(failed_batches)}")
          print(f"  Batches remaining: {len(pending_batches)}")
          print(f"  Total alerts processed: {sum(b['alert_count'] for b in completed_batches)}")
          print(f"  Total alerts failed: {sum(b['alert_count'] for b in failed_batches)}")
          print(f"  Alerts fixed (confirmed): {cursor.get('total_fixed', 0)}")
          print(f"  Alerts attempted (pending PR merge): {len(attempted_ids)}")
          print(f"  Alerts unfixable (human review): {len(unfixable_ids)}")
          if attempted_ids:
              print(f"  Attempted alert IDs: {attempted_ids}")
          if unfixable_ids:
              print(f"  Unfixable alert IDs: {unfixable_ids}")
          batch_prs = [b.get("pr_url") for b in completed_batches if b.get("pr_url")]
          if batch_prs:
              print(f"  PRs created:")
              for pr in batch_prs:
                  print(f"    - {pr}")

          # Write summary for GitHub Actions
          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"completed_count={len(completed_batches)}\n")
              f.write(f"failed_count={len(failed_batches)}\n")
              f.write(f"remaining_count={len(pending_batches)}\n")
              f.write(f"total_time={total_time}\n")
              f.write(f"attempted_count={len(attempted_ids)}\n")
              f.write(f"attempted_alert_ids={','.join(str(a) for a in attempted_ids)}\n")
              f.write(f"unfixable_count={len(unfixable_ids)}\n")
              f.write(f"unfixable_alert_ids={','.join(str(a) for a in unfixable_ids)}\n")

          # Write detailed results (Bug #28 fix: include pr_urls for step summary)
          batch_pr_urls = [b.get("pr_url") for b in completed_batches if b.get("pr_url")]
          results = {
              "completed": [{"batch_id": b["batch_id"], "alert_ids": b["alert_ids"]} for b in completed_batches],
              "failed": [{"batch_id": b.get("batch_id"), "conclusion": b.get("conclusion"), "alert_ids": b.get("alert_ids", [])} for b in failed_batches],
              "remaining": [{"batch_id": b["batch_id"]} for b in pending_batches],
              "attempted_alert_ids": attempted_ids,
              "unfixable_alert_ids": unfixable_ids,
              "pr_urls": batch_pr_urls
          }
          with open("/tmp/orchestrator_results.json", "w") as f:
              json.dump(results, f, indent=2)

          ORCHESTRATE_EOF

      # ----------------------------------------------------------------
      # STEP 7: Summary
      # ----------------------------------------------------------------
      - name: Summary
        if: always()
        run: |
          echo "## Devin Security Backlog Sweep" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total alerts on main**: ${{ steps.fetch-alerts.outputs.total_alerts }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Batches created**: ${{ steps.batch-alerts.outputs.batch_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Batches completed**: ${{ steps.orchestrate.outputs.completed_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Batches failed**: ${{ steps.orchestrate.outputs.failed_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Batches remaining**: ${{ steps.orchestrate.outputs.remaining_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Alerts attempted (pending PR merge)**: ${{ steps.orchestrate.outputs.attempted_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Alerts unfixable (human review)**: ${{ steps.orchestrate.outputs.unfixable_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Total time**: ${{ steps.orchestrate.outputs.total_time }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Tracking issue**: #${{ steps.tracking-issue.outputs.issue_number }}" >> $GITHUB_STEP_SUMMARY

          # Bug #28 fix: Include PR URLs in step summary so enterprise teams
          # can start reviewing immediately from the Actions UI.
          if [ -f /tmp/orchestrator_results.json ]; then
            PR_URLS=$(python3 -c "import json; data=json.load(open('/tmp/orchestrator_results.json')); prs=data.get('pr_urls',[]); print('\n'.join(prs)) if prs else None" 2>/dev/null)
            if [ -n "$PR_URLS" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### Fix PRs Created" >> $GITHUB_STEP_SUMMARY
              echo "$PR_URLS" | while read -r url; do
                echo "- $url" >> $GITHUB_STEP_SUMMARY
              done
            fi
          fi

      # ----------------------------------------------------------------
      # STEP 8: Report unfixable alerts for human review
      # ----------------------------------------------------------------
      - name: Report unfixable alerts
        if: always() && steps.orchestrate.outputs.unfixable_count != '0' && steps.orchestrate.outputs.unfixable_count != ''
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          REPO="${{ github.repository }}"
          ISSUE_NUMBER="${{ steps.tracking-issue.outputs.issue_number }}"
          UNFIXABLE_IDS="${{ steps.orchestrate.outputs.unfixable_alert_ids }}"
          UNFIXABLE_COUNT="${{ steps.orchestrate.outputs.unfixable_count }}"
          RUN_ID="${{ github.run_id }}"

          # Build the human review comment body
          BODY="## :warning: Alerts Requiring Human Review\n\n"
          BODY+="**${UNFIXABLE_COUNT} alert(s)** could not be automatically fixed by Devin. The files containing these alerts were not modified by the Devin session, indicating the fix was not attempted or was too complex for automated resolution.\n\n"
          BODY+="These alerts need manual investigation by a security engineer.\n\n"
          BODY+="| Alert | Link | Action Needed |\n"
          BODY+="|-------|------|---------------|\n"

          IFS=',' read -ra AIDS <<< "$UNFIXABLE_IDS"
          for aid in "${AIDS[@]}"; do
            if [ -n "$aid" ]; then
              BODY+="| #${aid} | [View alert](https://github.com/${REPO}/security/code-scanning/${aid}) | :eyes: Manual review |\n"
            fi
          done

          BODY+="\n---\n"
          BODY+="*Reported by [orchestrator run](https://github.com/${REPO}/actions/runs/${RUN_ID})*\n"
          BODY+="*These alerts were attempted by Devin but could not be resolved. Possible reasons: complex logic requiring human judgment, false positives that should be dismissed, or alerts requiring architectural changes.*"

          # Post comment on the tracking issue
          if [ -n "$ISSUE_NUMBER" ] && [ "$ISSUE_NUMBER" != "0" ]; then
            echo "Posting unfixable alerts report to issue #$ISSUE_NUMBER..."
            curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$ISSUE_NUMBER/comments" \
              -d "$(jq -n --arg body "$(echo -e "$BODY")" '{body: $body}')"
            echo "Posted unfixable alerts report."
          fi

          # Apply label for visibility
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/labels/devin%3Ahuman-review-needed")
          if [ "$HTTP_CODE" = "404" ]; then
            curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/labels" \
              -d '{"name":"devin:human-review-needed","color":"d93f0b","description":"Alerts that Devin could not auto-fix — requires human security review"}'
          fi
          curl -s -L -X POST \
            -H "Authorization: token $GH_PAT" \
            -H "Accept: application/vnd.github+json" \
            "https://api.github.com/repos/$REPO/issues/$ISSUE_NUMBER/labels" \
            -d '["devin:human-review-needed"]'
          echo "Applied devin:human-review-needed label to issue #$ISSUE_NUMBER"
