# Devin Security Backlog Sweep — Orchestrator Workflow
#
# This workflow processes the ENTIRE backlog of open CodeQL alerts on main.
# It uses a sub-workflow fan-out pattern:
#   1. Fetches all open CodeQL alerts on main (paginated)
#   2. Reads cursor from tracking issue (skip already-processed alerts)
#   3. Groups remaining alerts into batches (15 per batch, grouped by file)
#   4. Dispatches child workflows (devin-security-batch.yml), one per batch
#   5. Polls child workflow statuses (rolling window of max 3 active)
#   6. As each child completes, updates cursor
#   7. Continues until all batches are dispatched and completed
#
# KEY DESIGN DECISIONS (see DESIGN.md for full rationale):
#
# 1. SUB-WORKFLOW FAN-OUT: Each batch is a separate workflow run with its own
#    logs, timeout, and failure isolation. No 6-hour timeout concern.
#
# 2. STREAMING PR CREATION: PRs are created by child workflows as they complete,
#    not waiting for all batches. First PR available in ~10 minutes.
#
# 3. SESSION RESERVATION: Self-limits to 3 concurrent children, reserving 2
#    slots for PR workflows that may fire at any time.
#
# 4. CURSOR-BASED STATELESS PICKUP: A GitHub issue comment stores processed
#    alert IDs so the next run skips already-handled alerts.
#
# 5. WAVE-BASED PROCESSING: The orchestrator fills available slots, polls for
#    completion, and immediately backfills freed slots with pending batches.

name: Devin Security Backlog

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      max_batches:
        description: "Maximum number of batches to process (0 = unlimited)"
        required: false
        type: string
        default: "0"
      dry_run:
        description: "Log what would be done without dispatching children"
        required: false
        type: boolean
        default: false
      max_concurrent:
        description: "Maximum concurrent child workflows (default: 3, max: 5)"
        required: false
        type: string
        default: "3"
      reset_cursor:
        description: "Reset cursor and reprocess all alerts"
        required: false
        type: boolean
        default: false
      dispatch_ref:
        description: "Branch ref for child workflow dispatch (default: current branch)"
        required: false
        type: string
        default: ""

permissions:
  contents: write
  security-events: read
  pull-requests: write
  issues: write
  actions: write

concurrency:
  group: devin-security-backlog
  cancel-in-progress: false

jobs:
  orchestrate:
    runs-on: ubuntu-latest
    env:
      MAX_CONCURRENT: ${{ github.event.inputs.max_concurrent || '3' }}
      MAX_BATCHES: ${{ github.event.inputs.max_batches || '0' }}
      DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
      RESET_CURSOR: ${{ github.event.inputs.reset_cursor || 'false' }}
      ALERTS_PER_BATCH: 15
      POLL_INTERVAL: 60
      MAX_CHILD_RUNTIME: 3600
      SAFETY_TIMEOUT: 19800
      DISPATCH_REF: ${{ github.event.inputs.dispatch_ref || github.ref_name }}

    steps:
      - uses: actions/checkout@v4

      # ----------------------------------------------------------------
      # STEP 1: Health check
      # ----------------------------------------------------------------
      - name: Health check
        id: health-check
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
        run: |
          HEALTHY=true

          if [ -z "$GH_PAT" ]; then
            echo "::error::GH_PAT secret is not set."
            HEALTHY=false
          fi

          if [ -z "$DEVIN_API_KEY" ]; then
            echo "::error::DEVIN_API_KEY secret is not set."
            HEALTHY=false
          else
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
              -X GET "https://api.devin.ai/v1/sessions" \
              -H "Authorization: Bearer $DEVIN_API_KEY")
            if [ "$HTTP_CODE" = "401" ] || [ "$HTTP_CODE" = "403" ]; then
              echo "::error::DEVIN_API_KEY returned HTTP $HTTP_CODE"
              HEALTHY=false
            else
              echo "Devin API: reachable (HTTP $HTTP_CODE)"
            fi
          fi

          if [ "$HEALTHY" = "false" ]; then
            exit 1
          fi
          echo "Health check passed."

      # ----------------------------------------------------------------
      # STEP 2: Fetch all open CodeQL alerts on main
      # ----------------------------------------------------------------
      - name: Fetch all open CodeQL alerts
        id: fetch-alerts
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          REPO="${{ github.repository }}"
          echo "Fetching all open CodeQL alerts on main..."

          PAGE=1
          echo '[]' > /tmp/all_alerts.json
          TOTAL_FETCHED=0

          while true; do
            RESP=$(curl -s -L \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/code-scanning/alerts?ref=refs/heads/main&state=open&per_page=100&page=$PAGE")

            PAGE_COUNT=$(echo "$RESP" | jq 'if type == "array" then length else 0 end')
            if [ "$PAGE_COUNT" -eq 0 ] || [ "$PAGE_COUNT" = "null" ]; then
              break
            fi

            echo "$RESP" > /tmp/alerts_page.json
            jq -s '.[0] + .[1]' /tmp/all_alerts.json /tmp/alerts_page.json > /tmp/alerts_merged.json
            mv /tmp/alerts_merged.json /tmp/all_alerts.json
            TOTAL_FETCHED=$((TOTAL_FETCHED + PAGE_COUNT))

            echo "Page $PAGE: $PAGE_COUNT alerts (total: $TOTAL_FETCHED)"

            if [ "$PAGE_COUNT" -lt 100 ]; then
              break
            fi
            PAGE=$((PAGE + 1))
          done

          echo "Total open alerts on main: $TOTAL_FETCHED"
          echo "total_alerts=$TOTAL_FETCHED" >> $GITHUB_OUTPUT

          if [ "$TOTAL_FETCHED" -eq 0 ]; then
            echo "No open alerts to process."
            echo "skip=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "skip=false" >> $GITHUB_OUTPUT

      # ----------------------------------------------------------------
      # STEP 3: Find or create tracking issue for cursor state
      # ----------------------------------------------------------------
      - name: Find or create tracking issue
        id: tracking-issue
        if: steps.fetch-alerts.outputs.skip != 'true'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          REPO="${{ github.repository }}"
          LABEL="devin:backlog-tracker"

          # Create label if it doesn't exist
          LABEL_HTTP=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/labels/$(echo "$LABEL" | sed 's/:/%3A/g')")
          if [ "$LABEL_HTTP" = "404" ]; then
            curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/labels" \
              -d "{\"name\": \"$LABEL\", \"color\": \"0075ca\", \"description\": \"Tracks Devin security backlog sweep progress\"}" \
              2>/dev/null || true
          fi

          # Find existing tracking issue
          ISSUES=$(curl -s -L \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/issues?labels=$LABEL&state=open&per_page=1")
          ISSUE_NUM=$(echo "$ISSUES" | jq -r '.[0].number // empty')

          if [ -z "$ISSUE_NUM" ]; then
            echo "Creating tracking issue..."
            RESP=$(curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues" \
              -d "{
                \"title\": \"Devin Security Backlog — Progress Tracker\",
                \"body\": \"This issue tracks the progress of the Devin Security Backlog Sweep.\\n\\nDo not edit this issue manually — it is updated automatically by the workflow.\",
                \"labels\": [\"$LABEL\"]
              }")
            ISSUE_NUM=$(echo "$RESP" | jq -r '.number')
            echo "Created tracking issue #$ISSUE_NUM"
          else
            echo "Found tracking issue #$ISSUE_NUM"
          fi

          echo "issue_number=$ISSUE_NUM" >> $GITHUB_OUTPUT

      # ----------------------------------------------------------------
      # STEP 4: Read cursor from tracking issue
      # ----------------------------------------------------------------
      - name: Read cursor
        id: read-cursor
        if: steps.fetch-alerts.outputs.skip != 'true'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          ISSUE_NUM: ${{ steps.tracking-issue.outputs.issue_number }}
        run: |
          REPO="${{ github.repository }}"

          if [ "$RESET_CURSOR" = "true" ]; then
            echo "Cursor reset requested. Starting fresh."
            echo '{"processed_alert_ids":[],"unfixable_alert_ids":[],"attempted_alert_ids":[],"in_progress":{},"total_fixed":0,"total_unfixable":0,"total_attempted":0}' > /tmp/cursor.json
            echo "cursor_found=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Fetch comments and find cursor
          COMMENTS=$(curl -s -L \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/issues/$ISSUE_NUM/comments?per_page=100")

          # Save comments for Python to read
          echo "$COMMENTS" > /tmp/cursor_comments.json

          python3 << 'CURSOR_PARSE_EOF'
          import json, os

          try:
              with open("/tmp/cursor_comments.json") as f:
                  comments = json.load(f)
                  if not isinstance(comments, list):
                      comments = []
          except:
              comments = []

          cursor = None
          comment_id = ""
          marker = "<!-- backlog-cursor -->"

          for c in comments:
              body = c.get("body", "")
              if marker in body:
                  comment_id = str(c.get("id", ""))
                  start = body.find("```json\n")
                  end = body.find("\n```", start + 8) if start >= 0 else -1
                  if start >= 0 and end >= 0:
                      json_str = body[start + 8:end]
                      try:
                          cursor = json.loads(json_str)
                      except:
                          print("WARNING: Cursor JSON corrupted. Starting fresh.")

          if cursor is None:
              cursor = {
                  "processed_alert_ids": [],
                  "unfixable_alert_ids": [],
                  "attempted_alert_ids": [],
                  "in_progress": {},
                  "total_fixed": 0,
                  "total_unfixable": 0,
                  "total_attempted": 0
              }
              print("No cursor found. Starting fresh scan.")
          else:
              print(f"Cursor loaded: {len(cursor.get('processed_alert_ids', []))} processed, {len(cursor.get('unfixable_alert_ids', []))} unfixable, {len(cursor.get('attempted_alert_ids', []))} attempted")

          with open("/tmp/cursor.json", "w") as f:
              json.dump(cursor, f, indent=2)

          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"cursor_comment_id={comment_id}\n")
              f.write(f"cursor_found={'true' if comment_id else 'false'}\n")
          CURSOR_PARSE_EOF

      # ----------------------------------------------------------------
      # STEP 5: Filter alerts and group into batches
      # ----------------------------------------------------------------
      - name: Filter and batch alerts
        id: batch-alerts
        if: steps.fetch-alerts.outputs.skip != 'true'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          python3 << 'BATCH_EOF'
          import json, os, time

          alerts_per_batch = int(os.environ.get("ALERTS_PER_BATCH", "15"))
          max_batches_str = os.environ.get("MAX_BATCHES", "0")
          max_batches = int(max_batches_str) if max_batches_str else 0

          with open("/tmp/all_alerts.json") as f:
              all_alerts = json.load(f)

          with open("/tmp/cursor.json") as f:
              cursor = json.load(f)

          processed = set(cursor.get("processed_alert_ids", []))
          unfixable = set(cursor.get("unfixable_alert_ids", []))
          attempted = set(cursor.get("attempted_alert_ids", []))

          # Bug #18b fix: Re-verify unfixable alerts before skipping them.
          # After a fix PR is merged and CodeQL re-runs, previously "unfixable"
          # alerts may now be fixed. Re-query CodeQL to check.
          import urllib.request, urllib.error
          HTTP_TIMEOUT = 30
          gh_pat = os.environ.get("GH_PAT", "")
          repo_name = os.environ.get("GITHUB_REPOSITORY", "")
          re_headers = {
              "Authorization": f"token {gh_pat}",
              "Accept": "application/vnd.github+json"
          }

          reverified_fixed = []
          if unfixable:
              print(f"\nRe-verifying {len(unfixable)} unfixable alerts...")
              for uid in list(unfixable):
                  try:
                      url = f"https://api.github.com/repos/{repo_name}/code-scanning/alerts/{uid}"
                      req = urllib.request.Request(url, headers=re_headers)
                      with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
                          alert_data = json.loads(resp.read())
                      state = alert_data.get("state", "open")
                      if state in ("fixed", "dismissed"):
                          reverified_fixed.append(uid)
                          print(f"  Alert #{uid}: now {state} on main -> moving to processed")
                      else:
                          print(f"  Alert #{uid}: still {state} -> remains unfixable")
                  except Exception as e:
                      print(f"  Alert #{uid}: re-verify error ({e}) -> remains unfixable")
                  time.sleep(0.3)

          # Also re-verify attempted alerts (fix PRs may have been merged)
          reverified_attempted = []
          if attempted:
              print(f"\nRe-verifying {len(attempted)} attempted alerts...")
              for uid in list(attempted):
                  try:
                      url = f"https://api.github.com/repos/{repo_name}/code-scanning/alerts/{uid}"
                      req = urllib.request.Request(url, headers=re_headers)
                      with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
                          alert_data = json.loads(resp.read())
                      state = alert_data.get("state", "open")
                      if state in ("fixed", "dismissed"):
                          reverified_attempted.append(uid)
                          print(f"  Alert #{uid}: now {state} on main -> moving to processed")
                      else:
                          print(f"  Alert #{uid}: still {state} -> remains attempted")
                  except Exception as e:
                      print(f"  Alert #{uid}: re-verify error ({e}) -> remains attempted")
                  time.sleep(0.3)

          # Apply re-verification results to cursor
          for uid in reverified_fixed:
              unfixable.discard(uid)
              processed.add(uid)
              if uid not in cursor["processed_alert_ids"]:
                  cursor["processed_alert_ids"].append(uid)
              if uid in cursor.get("unfixable_alert_ids", []):
                  cursor["unfixable_alert_ids"].remove(uid)
          for uid in reverified_attempted:
              attempted.discard(uid)
              processed.add(uid)
              if uid not in cursor["processed_alert_ids"]:
                  cursor["processed_alert_ids"].append(uid)
              if uid in cursor.get("attempted_alert_ids", []):
                  cursor["attempted_alert_ids"].remove(uid)

          if reverified_fixed or reverified_attempted:
              print(f"\nRe-verification results: {len(reverified_fixed)} unfixable->fixed, {len(reverified_attempted)} attempted->fixed")
              cursor["total_unfixable"] = len(cursor.get("unfixable_alert_ids", []))
              cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))
              # Save updated cursor so orchestrator step sees re-verified state
              with open("/tmp/cursor.json", "w") as f:
                  json.dump(cursor, f, indent=2)

          # Filter out already-processed, unfixable, and attempted alerts
          remaining = []
          skipped_processed = 0
          skipped_unfixable = 0
          skipped_attempted = 0

          for a in all_alerts:
              alert_num = a.get("number")
              if alert_num in processed:
                  skipped_processed += 1
                  continue
              if alert_num in unfixable:
                  skipped_unfixable += 1
                  continue
              if alert_num in attempted:
                  skipped_attempted += 1
                  continue
              remaining.append(a)

          print(f"\nTotal open alerts: {len(all_alerts)}")
          print(f"Already processed (confirmed fixed): {skipped_processed}")
          print(f"Unfixable (skipped, needs human review): {skipped_unfixable}")
          print(f"Attempted (skipped, pending PR merge): {skipped_attempted}")
          print(f"Remaining to process: {len(remaining)}")

          if not remaining:
              print("No new alerts to process.")
              gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
              with open(gh_out, "a") as f:
                  f.write("batch_count=0\n")
                  f.write("total_remaining=0\n")
              with open("/tmp/batches.json", "w") as f:
                  json.dump([], f)
              exit(0)

          # Group by file for efficient Devin sessions
          severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
          file_groups = {}
          for a in remaining:
              loc = a.get("most_recent_instance", {}).get("location", {})
              path = loc.get("path", "unknown")
              file_groups.setdefault(path, []).append(a)

          # Sort each file group by severity
          for fg in file_groups.values():
              fg.sort(key=lambda a: severity_order.get(
                  a.get("rule", {}).get("security_severity_level", "low"), 3))

          # Sort files by most severe alert
          sorted_files = sorted(
              file_groups.items(),
              key=lambda kv: min(severity_order.get(
                  a.get("rule", {}).get("security_severity_level", "low"), 3) for a in kv[1])
          )

          # Build batches: ALL alerts per file stay together (never split a file)
          #
          # WHY: If a batch touches file X but only fixes 3 of 10 alerts in X,
          # the PR modifies X and CodeQL reports the remaining 7 as failures.
          # By keeping all alerts for a file in the same batch, the PR either
          # cleans the file completely or doesn't touch it at all.
          #
          # STRATEGY: Pack whole-file groups into batches using first-fit.
          # A batch can exceed alerts_per_batch if a single file has more alerts
          # than the cap (rare but possible). We never break a file across batches.
          batches = []
          file_remaining = {}
          for fp, fa in sorted_files:
              file_remaining[fp] = list(fa)

          current_batch = []
          current_batch_files = set()
          for fp, _ in sorted_files:
              if not file_remaining.get(fp):
                  continue
              file_alerts = file_remaining[fp]
              if not file_alerts:
                  continue

              # If adding this file would exceed the cap AND we already have alerts,
              # close the current batch and start a new one
              if current_batch and len(current_batch) + len(file_alerts) > alerts_per_batch:
                  batch_id = len(batches) + 1
                  alert_ids = [a.get("number") for a in current_batch]
                  ts = int(time.time())
                  batches.append({
                      "batch_id": batch_id,
                      "alert_ids": alert_ids,
                      "alert_count": len(alert_ids),
                      "files": list(current_batch_files),
                      "branch_name": f"devin/security-batch-{batch_id}-{ts}"
                  })
                  current_batch = []
                  current_batch_files = set()

              # Add ALL alerts for this file (never split)
              current_batch.extend(file_alerts)
              current_batch_files.add(fp)
              file_remaining[fp] = []

          # Flush remaining batch
          if current_batch:
              batch_id = len(batches) + 1
              alert_ids = [a.get("number") for a in current_batch]
              ts = int(time.time())
              batches.append({
                  "batch_id": batch_id,
                  "alert_ids": alert_ids,
                  "alert_count": len(alert_ids),
                  "files": list(current_batch_files),
                  "branch_name": f"devin/security-batch-{batch_id}-{ts}"
              })

          # Apply max_batches limit
          if max_batches > 0:
              batches = batches[:max_batches]
              print(f"Limited to {max_batches} batches (per input)")

          print(f"\nCreated {len(batches)} batches:")
          for b in batches:
              print(f"  Batch {b['batch_id']}: {b['alert_count']} alerts in {', '.join(b['files'][:3])}")

          with open("/tmp/batches.json", "w") as f:
              json.dump(batches, f, indent=2)

          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"batch_count={len(batches)}\n")
              f.write(f"total_remaining={len(remaining)}\n")
          BATCH_EOF

      # ----------------------------------------------------------------
      # STEP 5b: Parse CodeQL configuration (for batch session prompts)
      # The orchestrator needs the repo's CodeQL config to build
      # session prompts when creating sessions in batch mode.
      # ----------------------------------------------------------------
      - name: Parse CodeQL configuration
        id: codeql-config
        if: steps.batch-alerts.outputs.batch_count != '0'
        run: |
          python3 -u << 'CONFIG_EOF'
          import yaml, json, os, glob

          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")

          codeql_file = None
          for pattern in [".github/workflows/codeql*.yml", ".github/workflows/codeql*.yaml",
                          ".github/workflows/code-scanning*.yml", ".github/workflows/code-scanning*.yaml"]:
              matches = glob.glob(pattern)
              if matches:
                  codeql_file = matches[0]
                  break

          if not codeql_file:
              print("No CodeQL workflow found — using defaults")
              with open(gh_out, "a") as f:
                  f.write("languages=python\n")
                  f.write("query_suite=security-and-quality\n")
                  f.write("threat_models=remote,local\n")
                  f.write("codeql_config_source=defaults\n")
              raise SystemExit(0)

          print(f"Found CodeQL workflow: {codeql_file}")

          with open(codeql_file) as f:
              workflow = yaml.safe_load(f)

          languages = set()
          for job_name, job in workflow.get("jobs", {}).items():
              matrix = job.get("strategy", {}).get("matrix", {})
              for item in matrix.get("include", []):
                  if "language" in item:
                      languages.add(item["language"])
              lang_list = matrix.get("language", [])
              if isinstance(lang_list, list):
                  languages.update(lang_list)
              elif isinstance(lang_list, str):
                  languages.add(lang_list)
              for step in job.get("steps", []):
                  uses = step.get("uses", "")
                  if "codeql-action/init" in uses:
                      with_block = step.get("with", {})
                      langs_str = with_block.get("languages", "")
                      if langs_str and ("$" + "{{") not in str(langs_str):
                          for lang in str(langs_str).split(","):
                              languages.add(lang.strip())

          query_suite = ""
          threat_models = set()
          for job_name, job in workflow.get("jobs", {}).items():
              for step in job.get("steps", []):
                  uses = step.get("uses", "")
                  if "codeql-action/init" in uses:
                      with_block = step.get("with", {})
                      queries = with_block.get("queries", "")
                      if queries:
                          query_suite = str(queries)
                      config_str = with_block.get("config", "")
                      if config_str:
                          try:
                              config = yaml.safe_load(str(config_str))
                              if isinstance(config, dict):
                                  for tm in config.get("threat-models", []):
                                      threat_models.add(str(tm))
                          except Exception as e:
                              print(f"  Warning: could not parse config block: {e}")

          if not languages:
              languages = {"python"}
          if not query_suite:
              query_suite = "security-and-quality"
          if not threat_models:
              threat_models = {"remote", "local"}

          languages_str = ",".join(sorted(languages))
          threat_models_str = ",".join(sorted(threat_models))

          print(f"  Languages: {languages_str}")
          print(f"  Query suite: {query_suite}")
          print(f"  Threat models: {threat_models_str}")

          with open(gh_out, "a") as f:
              f.write(f"languages={languages_str}\n")
              f.write(f"query_suite={query_suite}\n")
              f.write(f"threat_models={threat_models_str}\n")
              f.write(f"codeql_config_source={codeql_file}\n")
          CONFIG_EOF

      # ----------------------------------------------------------------
      # STEP 6: Batch-create Devin sessions and dispatch child workflows
      #
      # BATCH MODE: The orchestrator creates Devin sessions up-front for
      # all batches in the current wave. This means all sessions in a wave
      # start working simultaneously, rather than waiting for each child
      # workflow to create its own session sequentially.
      #
      # Flow: create N sessions (batch) → dispatch N children with session_ids
      #       → poll children → when child completes, backfill next batch
      #
      # This is the "batch session creation" pattern recommended by Devin's
      # Advanced Mode. Each session gets its own child workflow for isolation
      # (polling, CodeQL verification, PR creation, artifact upload).
      # ----------------------------------------------------------------
      - name: Dispatch and manage child workflows
        id: orchestrate
        if: steps.batch-alerts.outputs.batch_count != '0'
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
          ISSUE_NUM: ${{ steps.tracking-issue.outputs.issue_number }}
          CURSOR_COMMENT_ID: ${{ steps.read-cursor.outputs.cursor_comment_id }}
          CODEQL_LANGUAGES: ${{ steps.codeql-config.outputs.languages }}
          CODEQL_QUERY_SUITE: ${{ steps.codeql-config.outputs.query_suite }}
          CODEQL_THREAT_MODELS: ${{ steps.codeql-config.outputs.threat_models }}
          CODEQL_CONFIG_SOURCE: ${{ steps.codeql-config.outputs.codeql_config_source }}
          PYTHONUNBUFFERED: "1"
        run: |
          python3 << 'ORCHESTRATE_EOF'
          import json, os, sys, time, urllib.request, urllib.error

          # Force unbuffered stdout so logs appear in real-time (Bug #11 fix)
          sys.stdout.reconfigure(line_buffering=True)

          HTTP_TIMEOUT = 30  # seconds — prevent hanging on slow API responses (Bug #12 fix)

          repo = os.environ.get("GITHUB_REPOSITORY", "")
          gh_pat = os.environ["GH_PAT"]
          devin_key = os.environ["DEVIN_API_KEY"]
          issue_num = os.environ.get("ISSUE_NUM", "")
          cursor_comment_id = os.environ.get("CURSOR_COMMENT_ID", "")
          dry_run = os.environ.get("DRY_RUN", "false") == "true"
          max_concurrent = min(int(os.environ.get("MAX_CONCURRENT", "3")), 5)
          poll_interval = int(os.environ.get("POLL_INTERVAL", "60"))
          max_child_runtime = int(os.environ.get("MAX_CHILD_RUNTIME", "3600"))
          safety_timeout = int(os.environ.get("SAFETY_TIMEOUT", "19800"))
          run_id = os.environ.get("GITHUB_RUN_ID", "")

          with open("/tmp/batches.json") as f:
              all_batches = json.load(f)
          with open("/tmp/cursor.json") as f:
              cursor = json.load(f)

          if not all_batches:
              print("No batches to process.")
              exit(0)

          print(f"Orchestrator config:")
          print(f"  Batches: {len(all_batches)}")
          print(f"  Max concurrent: {max_concurrent}")
          print(f"  Poll interval: {poll_interval}s")
          print(f"  Max child runtime: {max_child_runtime}s ({max_child_runtime//60} min)")
          print(f"  Safety timeout: {safety_timeout}s ({safety_timeout//60} min)")
          print(f"  Dry run: {dry_run}")

          def gh_api(method, url, data=None):
              req = urllib.request.Request(
                  url,
                  data=json.dumps(data).encode() if data else None,
                  headers={
                      "Authorization": f"token {gh_pat}",
                      "Accept": "application/vnd.github+json",
                      "Content-Type": "application/json"
                  },
                  method=method
              )
              try:
                  resp = urllib.request.urlopen(req, timeout=HTTP_TIMEOUT)
                  body = resp.read()
                  status_code = resp.status
                  if not body or status_code == 204:
                      return {}, status_code
                  return json.loads(body), status_code
              except urllib.error.HTTPError as e:
                  body = e.read().decode()[:500]
                  return {"error": body, "status": e.code}, e.code
              except Exception as e:
                  return {"error": str(e)}, 0

          def check_active_devin_sessions():
              try:
                  req = urllib.request.Request(
                      "https://api.devin.ai/v1/sessions?status_in=running,started",
                      headers={"Authorization": f"Bearer {devin_key}"}
                  )
                  resp = urllib.request.urlopen(req, timeout=HTTP_TIMEOUT)
                  data = json.loads(resp.read())
                  sessions = data.get("sessions", [])
                  count = len(sessions)
                  print(f"  Active Devin sessions: {count}")
                  return count
              except Exception as e:
                  print(f"  Warning: Could not check active sessions: {e}")
                  return 0

          # Shared headers for direct urllib requests (artifact download, etc.)
          # gh_api() has its own internal headers, but artifact download uses urllib directly.
          headers = {
              "Authorization": f"token {gh_pat}",
              "Accept": "application/vnd.github+json"
          }

          # Bug #20/#21 fix: GitHub artifact download URLs return 302 redirects
          # to Azure Blob Storage. urllib forwards the Authorization header to
          # Azure, causing 403 (bug #20). Custom redirect handler caused 400
          # (bug #21). Solution: use curl which natively strips auth headers
          # on cross-domain redirects (since curl 7.58+).
          def download_artifact_zip(url, gh_token):
              """Download artifact zip using curl (handles auth-stripping on redirect)."""
              import subprocess, tempfile
              tmp_path = tempfile.mktemp(suffix=".zip")
              result = subprocess.run(
                  ["curl", "-sL",
                   "-H", f"Authorization: token {gh_token}",
                   "-H", "Accept: application/vnd.github+json",
                   "-o", tmp_path,
                   "-w", "%{http_code}",
                   url],
                  capture_output=True, text=True, timeout=60
              )
              http_code = result.stdout.strip()
              if http_code == "200" and os.path.exists(tmp_path):
                  with open(tmp_path, "rb") as f:
                      data = f.read()
                  os.unlink(tmp_path)
                  return data
              else:
                  if os.path.exists(tmp_path):
                      os.unlink(tmp_path)
                  raise Exception(f"curl returned HTTP {http_code}: {result.stderr[:200]}")

          dispatch_ref = os.environ.get("DISPATCH_REF", "main")

          # CodeQL config for building Devin session prompts
          codeql_languages = os.environ.get("CODEQL_LANGUAGES", "python")
          codeql_query_suite = os.environ.get("CODEQL_QUERY_SUITE", "security-and-quality")
          codeql_threat_models = os.environ.get("CODEQL_THREAT_MODELS", "remote,local")
          codeql_config_source = os.environ.get("CODEQL_CONFIG_SOURCE", "defaults")

          def fetch_alert_details(alert_ids):
              """Fetch alert details from CodeQL API for prompt building."""
              details = []
              summary_lines = []
              for aid in alert_ids:
                  try:
                      url = f"https://api.github.com/repos/{repo}/code-scanning/alerts/{aid}"
                      result, status = gh_api("GET", url)
                      if status == 200:
                          loc = result.get("most_recent_instance", {}).get("location", {})
                          rule = result.get("rule", {})
                          detail = {
                              "number": aid,
                              "rule_id": rule.get("id", "unknown"),
                              "severity": rule.get("security_severity_level", "unknown"),
                              "description": rule.get("description", ""),
                              "file": loc.get("path", ""),
                              "start_line": loc.get("start_line", 0),
                              "end_line": loc.get("end_line", 0),
                              "message": result.get("most_recent_instance", {}).get("message", {}).get("text", "")
                          }
                          details.append(detail)
                          summary_lines.append(
                              f"- Alert #{aid}: [{detail['rule_id']}] {detail['severity']} in {detail['file']}:{detail['start_line']} — {detail['message'][:120]}"
                          )
                  except Exception as e:
                      print(f"    Warning: Could not fetch alert #{aid}: {e}")
                      summary_lines.append(f"- Alert #{aid}: (details unavailable)")
                  time.sleep(0.3)
              return details, "\n".join(summary_lines)

          def build_session_prompt(batch, alert_details_json, alert_summary):
              """Build the Devin session prompt with CodeQL config."""
              import textwrap
              # Bug #47 fix: filter out 'actions' (scans YAML only) when real code languages exist
              code_langs = [l for l in codeql_languages.split(",") if l != "actions"]
              primary_lang = code_langs[0] if code_langs else codeql_languages.split(",")[0]
              query_pack = f"codeql/{primary_lang}-queries:codeql-suites/{primary_lang}-{codeql_query_suite}.qls"
              threat_display = " AND ".join(codeql_threat_models.split(","))
              threat_flags = ""
              for tm in codeql_threat_models.split(","):
                  if tm.strip() != "remote":
                      threat_flags += f" --threat-model={tm.strip()}"

              sarif_check = (
                  "python3 -c \"import json; sarif=json.load(open('/tmp/results.sarif')); "
                  "results=[r for run in sarif['runs'] for r in run.get('results',[])]; "
                  "file_alerts=[r for r in results if any("
                  "loc.get('physicalLocation',{}).get('artifactLocation',{}).get('uri','').endswith('FILE') "
                  "for loc in r.get('locations',[]))]; "
                  "print(f'Found {len(file_alerts)} alerts in FILE'); "
                  "[print(f'  - {r[\\\"ruleId\\\"]}:{r.get(\\\"message\\\",{}).get(\\\"text\\\",\\\"\\\")[:80]}') for r in file_alerts]; "
                  "exit(1 if file_alerts else 0)\""
              )

              prompt = textwrap.dedent(f"""\
                  CRITICAL OPERATING MODE: You are running inside an UNATTENDED CI/CD pipeline with NO human operator. You MUST NOT use block_on_user=true, you MUST NOT ask questions, you MUST NOT wait for confirmation, you MUST NOT request approval. If you encounter ANY uncertainty, make your best judgment and continue. If you get stuck, skip the current alert and move to the next one. NEVER stop working to wait for input.

                  You are a security engineer fixing pre-existing CodeQL vulnerabilities in repository {repo}.

                  This is Batch #{batch['batch_id']} of an automated backlog sweep. Fix these alerts on a new branch.

                  Alerts to fix:

                  {alert_summary}

                  Detailed alert information:
                  {alert_details_json}

                  Instructions:
                  1. Clone the repository: https://github.com/{repo}.git
                  2. Create and checkout branch '{batch['branch_name']}' from main
                  3. For each alert, read the surrounding code and understand the full context
                  4. Fix each alert ONE AT A TIME. For each alert:
                     a. Apply a minimal, focused fix following the codebase's existing conventions
                     b. VERIFY the fix locally by running CodeQL CLI to confirm the alert is resolved.
                        IMPORTANT: You MUST use the EXACT same CodeQL configuration as the repo's CI.
                        This repo's CI config (parsed from {codeql_config_source}) uses:
                          - languages: {codeql_languages}
                          - queries: {codeql_query_suite}
                          - threat-models: {threat_display}
                        Your verification commands:
                        - Download and extract CodeQL CLI (only once, reuse for all alerts):
                          wget -q https://github.com/github/codeql-cli-binaries/releases/latest/download/codeql-linux64.zip && unzip -qo codeql-linux64.zip
                        - Create a fresh database (MUST use --overwrite since DB may exist from prior alert):
                          ./codeql/codeql database create /tmp/codeql-db --language={primary_lang} --source-root=. --overwrite
                        - Run analysis with the EXACT same query suite and threat models as CI:
                          ./codeql/codeql database analyze /tmp/codeql-db {query_pack} --format=sarif-latest --output=/tmp/results.sarif --download{threat_flags}
                        - Parse the SARIF output to check for ANY alerts in the file you modified:
                          {sarif_check}
                          (Replace FILE with the actual file path for each alert)
                     c. If ANY alert (old or new) still appears in the file after your fix, revise your approach and re-run CodeQL (max 2 attempts total)
                     d. If after 2 attempts alerts persist, SKIP this alert entirely — do NOT commit a broken fix. Note it as unfixable and move on immediately.
                     e. If the file is clean (CodeQL reports zero alerts for it), commit with message: fix: [rule_id] description (file:line)
                  5. Conventions to follow:
                     - If the project uses an ORM, use parameterized queries via the ORM
                     - If there are existing sanitization utilities, reuse them
                     - Do not introduce new dependencies unless absolutely necessary
                  6. If the repository has tests, run them to ensure no regressions
                  7. Push all commit(s) to branch '{batch['branch_name']}'
                  8. After processing ALL alerts, finish your session immediately. Do not wait.

                  IMPORTANT RULES — VIOLATION WILL BREAK THE PIPELINE:
                  - NEVER use block_on_user=true. NEVER ask questions. NEVER wait for input. This is fully automated.
                  - If you are unsure about ANYTHING, make your best judgment and continue. Do not stop.
                  - Each security issue MUST be a separate commit
                  - Do NOT just suppress or ignore alerts — fix the root cause
                  - Keep fixes minimal and surgical — do not refactor unrelated code
                  - If an alert cannot be fixed after 2 CodeQL verification attempts, SKIP it immediately
                  - Your fixes will be verified by the pipeline using the EXACT same CodeQL config as CI. If your fix introduces new alerts or does not resolve the target, the PR will be flagged.
                  - When done with all alerts, push your branch and end the session. Do not block.""")
              return prompt

          def create_devin_session(batch):
              """Create a Devin session for a batch via v1 API. Returns (session_id, session_url) or (None, None).
              Bug #49 fix: retries with exponential backoff on 429 before giving up.
              Previously, a single 429 caused immediate fallback to standalone mode,
              which meant the child would also hit 429 (same rate limit)."""
              print(f"  Creating Devin session for Batch {batch['batch_id']}...")
              details, summary = fetch_alert_details(batch["alert_ids"])
              details_json = json.dumps(details, indent=2)
              prompt = build_session_prompt(batch, details_json, summary)

              max_retries = 3
              retry_wait = 30
              for attempt in range(1, max_retries + 1):
                  try:
                      req = urllib.request.Request(
                          "https://api.devin.ai/v1/sessions",
                          data=json.dumps({"prompt": prompt, "max_acu_limit": 10}).encode(),
                          headers={
                              "Authorization": f"Bearer {devin_key}",
                              "Content-Type": "application/json"
                          },
                          method="POST"
                      )
                      resp = urllib.request.urlopen(req, timeout=HTTP_TIMEOUT)
                      data = json.loads(resp.read())
                      session_id = data.get("session_id", "")
                      session_url = data.get("url", "")
                      if not session_url and session_id:
                          session_url = f"https://app.devin.ai/sessions/{session_id}"
                      if session_id:
                          print(f"    Session created: {session_url}")
                          return session_id, session_url
                      else:
                          print(f"    Session creation returned no session_id")
                          return None, None
                  except urllib.error.HTTPError as e:
                      status_code = e.code
                      body = e.read().decode()[:300]
                      if status_code == 429 and attempt < max_retries:
                          print(f"    Rate limited (429) — retry {attempt}/{max_retries} in {retry_wait}s...")
                          time.sleep(retry_wait)
                          retry_wait *= 2
                          continue
                      elif status_code == 429:
                          print(f"    Rate limited (429) — all {max_retries} retries exhausted")
                      else:
                          print(f"    Session creation failed (HTTP {status_code}): {body}")
                      return None, None
                  except Exception as e:
                      print(f"    Session creation error: {e}")
                      return None, None
              return None, None

          def dispatch_child(batch, session_id=None, session_url=None):
              """Dispatch child workflow, optionally with pre-created session."""
              alert_ids_str = ",".join(str(a) for a in batch["alert_ids"])
              url = f"https://api.github.com/repos/{repo}/actions/workflows/devin-security-batch.yml/dispatches"
              inputs = {
                  "batch_id": str(batch["batch_id"]),
                  "alert_ids": alert_ids_str,
                  "branch_name": batch["branch_name"],
                  "tracking_issue": str(issue_num)
              }
              if session_id:
                  inputs["session_id"] = session_id
                  inputs["session_url"] = session_url or ""
              data = {"ref": dispatch_ref, "inputs": inputs}
              result, status = gh_api("POST", url, data)
              return status == 204

          def check_child_status(run_id_val):
              url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id_val}"
              result, status = gh_api("GET", url)
              if status != 200:
                  return "unknown"
              return result.get("status", "unknown"), result.get("conclusion")

          def update_cursor(cursor_data):
              global cursor_comment_id
              marker = "<!-- backlog-cursor -->"
              cursor_json = json.dumps(cursor_data, indent=2)
              body = f"{marker}\n## Backlog Sweep Cursor\n\n*Last updated: {time.strftime('%Y-%m-%d %H:%M UTC', time.gmtime())}*\n\n```json\n{cursor_json}\n```\n\n---\n*Updated by [orchestrator run](https://github.com/{repo}/actions/runs/{run_id})*"
              url_base = f"https://api.github.com/repos/{repo}/issues"

              if cursor_comment_id:
                  result, status = gh_api("PATCH", f"{url_base}/comments/{cursor_comment_id}", {"body": body})
                  if status == 200:
                      return True
              # Create new comment (Bug #26 fix: save the new comment ID so subsequent
              # calls PATCH instead of creating more comments)
              result, status = gh_api("POST", f"{url_base}/{issue_num}/comments", {"body": body})
              if status == 201 and result.get("id"):
                  new_id = str(result["id"])
                  # Bug #32 fix: Delete stale cursor comments from previous runs.
                  # This keeps the tracking issue clean with exactly one active cursor comment.
                  try:
                      comments_result, c_status = gh_api("GET", f"{url_base}/{issue_num}/comments?per_page=100")
                      if c_status == 200 and isinstance(comments_result, list):
                          for c in comments_result:
                              c_id = str(c.get("id", ""))
                              if c_id != new_id and marker in c.get("body", ""):
                                  _, del_status = gh_api("DELETE", f"{url_base}/comments/{c_id}")
                                  if del_status == 204:
                                      print(f"    Deleted stale cursor comment {c_id}")
                  except Exception as e:
                      print(f"    Warning: stale cursor cleanup failed ({e}), continuing")
                  cursor_comment_id = new_id
                  print(f"    Created cursor comment {cursor_comment_id}")
              return status == 201

          # ============================================================
          # MAIN ORCHESTRATOR LOOP — BATCH SESSION CREATION
          #
          # The orchestrator creates Devin sessions UP-FRONT for all
          # batches in the current wave. This is the "batch mode" pattern:
          # all sessions start working simultaneously instead of waiting
          # for each child workflow to create its own session sequentially.
          #
          # Flow per wave:
          #   1. Create N Devin sessions (batch creation)
          #   2. Dispatch N child workflows with session_ids
          #   3. Poll children until completion
          #   4. Backfill freed slots with next batch (create session + dispatch)
          # ============================================================
          pending_batches = list(all_batches)
          active_children = {}  # key -> {batch, dispatched_at, batch_id, session_id, ...}
          completed_batches = []
          failed_batches = []
          start_time = time.time()
          _counters = {"sessions_created": 0}

          if dry_run:
              print("\n=== DRY RUN MODE ===")
              for b in pending_batches:
                  print(f"  Would dispatch: Batch {b['batch_id']} ({b['alert_count']} alerts) -> {b['branch_name']}")
              print(f"\nTotal: {len(pending_batches)} batches, {sum(b['alert_count'] for b in pending_batches)} alerts")
              exit(0)

          print(f"\n{'='*60}")
          print(f"Starting orchestrator loop (BATCH MODE)")
          print(f"{'='*60}\n")

          def create_and_dispatch(batch):
              """Batch mode: create Devin session first, then dispatch child with session_id."""
              dispatch_time = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

              print(f"[BATCH] Creating session + dispatching Batch {batch['batch_id']} ({batch['alert_count']} alerts)...")

              session_id, session_url = create_devin_session(batch)
              if not session_id:
                  print(f"  Session creation failed — dispatching child without session (fallback)")
                  session_id = None
                  session_url = None
              else:
                  _counters["sessions_created"] += 1

              success = dispatch_child(batch, session_id=session_id, session_url=session_url)

              if success:
                  time.sleep(5)
                  active_children[f"batch_{batch['batch_id']}"] = {
                      "batch": batch,
                      "dispatched_at": time.time(),
                      "dispatch_time_str": dispatch_time,
                      "run_id": None,
                      "status": "dispatched",
                      "session_id": session_id,
                      "session_url": session_url
                  }
                  mode = "BATCH" if session_id else "STANDALONE"
                  print(f"  Dispatched successfully ({mode} mode)")
                  return True
              else:
                  print(f"  Dispatch FAILED — will retry later")
                  return False

          # Initial fill: create sessions + dispatch up to max_concurrent children
          print(f"=== WAVE 1: Batch-creating up to {max_concurrent} sessions ===\n")
          while len(active_children) < max_concurrent and pending_batches:
              batch = pending_batches.pop(0)
              if not create_and_dispatch(batch):
                  pending_batches.insert(0, batch)
                  time.sleep(10)

          print(f"\n=== Initial wave dispatched: {len(active_children)} children, {_counters['sessions_created']} sessions created in batch ===\n")

          # Poll loop
          poll_count = 0
          while active_children or pending_batches:
              elapsed = time.time() - start_time
              if elapsed > safety_timeout:
                  print(f"\n  SAFETY TIMEOUT ({safety_timeout//60} min). Saving cursor and exiting.")
                  print(f"  Completed: {len(completed_batches)}, Failed: {len(failed_batches)}, Remaining: {len(pending_batches) + len(active_children)}")
                  break

              time.sleep(poll_interval)
              poll_count += 1
              print(f"\n--- Poll #{poll_count} (elapsed: {int(elapsed)}s, active: {len(active_children)}, pending: {len(pending_batches)}, sessions_created: {_counters['sessions_created']}) ---")

              # Backfill: create session + dispatch for freed slots
              if pending_batches and len(active_children) < max_concurrent:
                  batch = pending_batches.pop(0)
                  print(f"  [BACKFILL] Batch {batch['batch_id']} ({batch['alert_count']} alerts)...")
                  if not create_and_dispatch(batch):
                      pending_batches.insert(0, batch)

              # Resolve run IDs for newly dispatched children
              # NOTE (Bug #22 fix): workflow_dispatch runs are returned newest-first
              # by the API. When multiple batches are dispatched close together,
              # iterating newest-first causes cross-matching (batch 1 grabs batch 2's
              # run). Fix: sort runs oldest-first so dispatch order matches run order.
              # Also resolve ALL unmatched children in one pass against the same
              # sorted run list to ensure correct 1:1 matching.
              unresolved = [(k, c) for k, c in active_children.items() if c["run_id"] is None]
              if unresolved:
                  # Use the earliest dispatch date for the query
                  earliest_date = min(c["dispatch_time_str"][:10] for _, c in unresolved)
                  url = f"https://api.github.com/repos/{repo}/actions/workflows/devin-security-batch.yml/runs?per_page=30&event=workflow_dispatch&created=>={earliest_date}"
                  result, status = gh_api("GET", url)
                  if status == 200:
                      already_matched = set(str(c.get("run_id")) for c in active_children.values() if c.get("run_id"))
                      # Sort oldest-first to match dispatch order
                      sorted_runs = sorted(result.get("workflow_runs", []), key=lambda r: r.get("created_at", ""))
                      # Sort unresolved children by dispatch time (oldest first)
                      unresolved_sorted = sorted(unresolved, key=lambda x: x[1]["dispatch_time_str"])
                      for key, child in unresolved_sorted:
                          for run in sorted_runs:
                              run_created = run.get("created_at", "")
                              run_id_val = run["id"]
                              if run_created >= child["dispatch_time_str"] and str(run_id_val) not in already_matched:
                                  child["run_id"] = run_id_val
                                  already_matched.add(str(run_id_val))
                                  print(f"  Resolved Batch {child['batch']['batch_id']} -> run {run_id_val} (timestamp match: created={run_created} >= dispatched={child['dispatch_time_str']})")
                                  break

              # Check status of active children
              for key, child in list(active_children.items()):
                  if child["run_id"] is None:
                      # Check for stuck dispatch
                      age = time.time() - child["dispatched_at"]
                      if age > 300:  # 5 minutes without finding run
                          print(f"  Batch {child['batch']['batch_id']}: No workflow run found after {int(age)}s — re-dispatching")
                          pending_batches.append(child["batch"])
                          del active_children[key]
                      continue

                  run_id_val = child["run_id"]
                  status_tuple = check_child_status(run_id_val)

                  if isinstance(status_tuple, tuple):
                      run_status, conclusion = status_tuple
                  else:
                      run_status, conclusion = status_tuple, None

                  child["status"] = run_status

                  if run_status == "completed":
                      batch = child["batch"]
                      if conclusion == "success":
                          print(f"  Batch {batch['batch_id']}: COMPLETED (success)")
                          completed_batches.append(batch)

                          # Fetch batch result artifact to get per-alert breakdown
                          # Bug #18 fix: Now includes attempted_alert_ids (file modified but not yet merged)
                          batch_unfixable = []
                          batch_fixed = []
                          batch_attempted = []
                          batch_pr_url = ""
                          artifact_fetched = False
                          try:
                              run_id_val = child["run_id"]
                              artifacts_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id_val}/artifacts"
                              req = urllib.request.Request(artifacts_url, headers=headers)
                              with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
                                  artifacts = json.loads(resp.read())
                              for artifact in artifacts.get("artifacts", []):
                                  if artifact["name"].startswith(f"batch-{batch['batch_id']}-"):
                                      print(f"    Found result artifact: {artifact['name']}")
                                      dl_url = artifact["archive_download_url"]
                                      import zipfile, io
                                      zip_data = download_artifact_zip(dl_url, gh_pat)
                                      z = zipfile.ZipFile(io.BytesIO(zip_data))
                                      for name in z.namelist():
                                          if name.endswith(".json"):
                                              result_data = json.loads(z.read(name))
                                              batch_fixed = [int(x) for x in result_data.get("fixed_alert_ids", [])]
                                              batch_attempted = [int(x) for x in result_data.get("attempted_alert_ids", [])]
                                              batch_unfixable = [int(x) for x in result_data.get("unfixable_alert_ids", [])]
                                              batch_pr_url = result_data.get("pr_url", "")
                                              artifact_fetched = True
                                              print(f"    Fixed: {len(batch_fixed)}, Attempted: {len(batch_attempted)}, Unfixable: {len(batch_unfixable)}, PR: {batch_pr_url or 'none'}")
                                      z.close()
                          except Exception as e:
                              print(f"    Could not fetch batch result artifact: {e}")
                              print(f"    Falling back: marking all {len(batch['alert_ids'])} alerts as attempted (not processed — status unknown)")

                          # Update cursor with three-state classification (Bug #18 fix)
                          if artifact_fetched and (batch_fixed or batch_attempted or batch_unfixable):
                              for aid in batch_fixed:
                                  if aid not in cursor["processed_alert_ids"]:
                                      cursor["processed_alert_ids"].append(aid)
                              for aid in batch_attempted:
                                  if aid not in cursor.get("attempted_alert_ids", []):
                                      cursor.setdefault("attempted_alert_ids", []).append(aid)
                              for aid in batch_unfixable:
                                  if aid not in cursor.get("unfixable_alert_ids", []):
                                      cursor.setdefault("unfixable_alert_ids", []).append(aid)
                              cursor["total_fixed"] = cursor.get("total_fixed", 0) + len(batch_fixed)
                              cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))
                              cursor["total_unfixable"] = len(cursor.get("unfixable_alert_ids", []))
                          else:
                              # Bug #24 fix: mark as attempted (not processed) when artifact is missing.
                              # "processed" implies confirmed fixed. Without artifact data, we don't know
                              # if the fix worked. Mark as attempted so the next run re-verifies.
                              for aid in batch["alert_ids"]:
                                  if aid not in cursor.get("attempted_alert_ids", []):
                                      cursor.setdefault("attempted_alert_ids", []).append(aid)
                              cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))
                          cursor["total_processed"] = len(cursor["processed_alert_ids"])

                          # Track batch PR URL for summary reporting (Bug #25 fix)
                          if batch_pr_url:
                              batch["pr_url"] = batch_pr_url
                      else:
                          print(f"  Batch {batch['batch_id']}: COMPLETED (conclusion={conclusion})")
                          failed_batches.append({**batch, "conclusion": conclusion})
                          # Bug #27 fix: Track failed batch alerts as "attempted" in cursor.
                          # Without this, the next orchestrator run treats them as unprocessed
                          # and re-dispatches them. Mark as attempted so re-verification can
                          # check if a partial fix landed, or a future run can detect them.
                          for aid in batch["alert_ids"]:
                              if aid not in cursor.get("attempted_alert_ids", []):
                                  cursor.setdefault("attempted_alert_ids", []).append(aid)
                          cursor["total_attempted"] = len(cursor.get("attempted_alert_ids", []))

                      del active_children[key]

                      # Backfill: create session + dispatch next pending batch (batch mode)
                      if pending_batches and len(active_children) < max_concurrent:
                              next_batch = pending_batches.pop(0)
                              print(f"  [BACKFILL] Batch {next_batch['batch_id']}...")
                              if not create_and_dispatch(next_batch):
                                  pending_batches.insert(0, next_batch)

                      # Update cursor after each completion (streaming)
                      update_cursor(cursor)

                  elif run_status in ("queued", "in_progress", "waiting"):
                      age = time.time() - child["dispatched_at"]
                      if age > max_child_runtime:
                          print(f"  Batch {child['batch']['batch_id']}: STALE ({int(age)}s > {max_child_runtime}s) — evicting")
                          failed_batches.append({**child["batch"], "conclusion": "timeout_evicted"})
                          del active_children[key]
                      else:
                          print(f"  Batch {child['batch']['batch_id']}: {run_status} ({int(age)}s)")

                  else:
                      # Bug #22 fix: handle unexpected status (e.g. "unknown" from API failure)
                      age = time.time() - child["dispatched_at"]
                      print(f"  Batch {child['batch']['batch_id']}: unexpected status '{run_status}' ({int(age)}s)")
                      if age > max_child_runtime:
                          print(f"  Batch {child['batch']['batch_id']}: evicting after {int(age)}s with status '{run_status}'")
                          failed_batches.append({**child["batch"], "conclusion": f"unknown_status_{run_status}"})
                          del active_children[key]

          # ============================================================
          # FINAL CURSOR UPDATE
          # ============================================================
          cursor["last_run"] = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
          cursor["last_run_id"] = run_id
          update_cursor(cursor)

          # Summary
          total_time = int(time.time() - start_time)
          unfixable_ids = cursor.get("unfixable_alert_ids", [])
          attempted_ids = cursor.get("attempted_alert_ids", [])
          print(f"\n{'='*60}")
          print(f"Orchestrator complete (BATCH MODE)")
          print(f"{'='*60}")
          print(f"  Mode: Batch session creation (sessions created up-front by orchestrator)")
          print(f"  Sessions created in batch: {_counters['sessions_created']}")
          print(f"  Total time: {total_time}s ({total_time//60} min)")
          print(f"  Batches completed: {len(completed_batches)}")
          print(f"  Batches failed: {len(failed_batches)}")
          print(f"  Batches remaining: {len(pending_batches)}")
          print(f"  Total alerts processed: {sum(b['alert_count'] for b in completed_batches)}")
          print(f"  Total alerts failed: {sum(b['alert_count'] for b in failed_batches)}")
          print(f"  Alerts fixed (confirmed): {cursor.get('total_fixed', 0)}")
          print(f"  Alerts attempted (pending PR merge): {len(attempted_ids)}")
          print(f"  Alerts unfixable (human review): {len(unfixable_ids)}")
          if attempted_ids:
              print(f"  Attempted alert IDs: {attempted_ids}")
          if unfixable_ids:
              print(f"  Unfixable alert IDs: {unfixable_ids}")
          batch_prs = [b.get("pr_url") for b in completed_batches if b.get("pr_url")]
          if batch_prs:
              print(f"  PRs created:")
              for pr in batch_prs:
                  print(f"    - {pr}")

          # Write summary for GitHub Actions
          gh_out = os.environ.get("GITHUB_OUTPUT", "/dev/null")
          with open(gh_out, "a") as f:
              f.write(f"completed_count={len(completed_batches)}\n")
              f.write(f"failed_count={len(failed_batches)}\n")
              f.write(f"remaining_count={len(pending_batches)}\n")
              f.write(f"total_time={total_time}\n")
              f.write(f"attempted_count={len(attempted_ids)}\n")
              f.write(f"attempted_alert_ids={','.join(str(a) for a in attempted_ids)}\n")
              f.write(f"unfixable_count={len(unfixable_ids)}\n")
              f.write(f"unfixable_alert_ids={','.join(str(a) for a in unfixable_ids)}\n")

          # Write detailed results (Bug #28 fix: include pr_urls for step summary)
          batch_pr_urls = [b.get("pr_url") for b in completed_batches if b.get("pr_url")]
          results = {
              "completed": [{"batch_id": b["batch_id"], "alert_ids": b["alert_ids"]} for b in completed_batches],
              "failed": [{"batch_id": b.get("batch_id"), "conclusion": b.get("conclusion"), "alert_ids": b.get("alert_ids", [])} for b in failed_batches],
              "remaining": [{"batch_id": b["batch_id"]} for b in pending_batches],
              "attempted_alert_ids": attempted_ids,
              "unfixable_alert_ids": unfixable_ids,
              "pr_urls": batch_pr_urls
          }
          with open("/tmp/orchestrator_results.json", "w") as f:
              json.dump(results, f, indent=2)

          ORCHESTRATE_EOF

      # ----------------------------------------------------------------
      # STEP 7: Summary
      # ----------------------------------------------------------------
      - name: Summary
        if: always()
        run: |
          BATCH_COUNT="${{ steps.batch-alerts.outputs.batch_count }}"
          BATCH_COUNT="${BATCH_COUNT:-0}"

          echo "## Devin Security Backlog Sweep" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total alerts on main**: ${{ steps.fetch-alerts.outputs.total_alerts || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Batches created**: $BATCH_COUNT" >> $GITHUB_STEP_SUMMARY

          if [ "$BATCH_COUNT" = "0" ]; then
            echo "- **Result**: No new alerts to process (all already attempted/processed/unfixable)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Batches completed**: ${{ steps.orchestrate.outputs.completed_count || '0' }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Batches failed**: ${{ steps.orchestrate.outputs.failed_count || '0' }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Batches remaining**: ${{ steps.orchestrate.outputs.remaining_count || '0' }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Alerts attempted (pending PR merge)**: ${{ steps.orchestrate.outputs.attempted_count || '0' }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Alerts unfixable (human review)**: ${{ steps.orchestrate.outputs.unfixable_count || '0' }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Total time**: ${{ steps.orchestrate.outputs.total_time || '0' }}s" >> $GITHUB_STEP_SUMMARY
          fi
          echo "- **Tracking issue**: #${{ steps.tracking-issue.outputs.issue_number || 'N/A' }}" >> $GITHUB_STEP_SUMMARY

          # Bug #28 fix: Include PR URLs in step summary so enterprise teams
          # can start reviewing immediately from the Actions UI.
          if [ -f /tmp/orchestrator_results.json ]; then
            PR_URLS=$(python3 -c "import json; data=json.load(open('/tmp/orchestrator_results.json')); prs=data.get('pr_urls',[]); print('\n'.join(prs)) if prs else None" 2>/dev/null)
            if [ -n "$PR_URLS" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### Fix PRs Created" >> $GITHUB_STEP_SUMMARY
              echo "$PR_URLS" | while read -r url; do
                echo "- $url" >> $GITHUB_STEP_SUMMARY
              done
            fi
          fi

      # ----------------------------------------------------------------
      # STEP 8: Report unfixable alerts for human review
      # ----------------------------------------------------------------
      - name: Report unfixable alerts
        if: always() && steps.orchestrate.outputs.unfixable_count != '0' && steps.orchestrate.outputs.unfixable_count != ''
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          REPO="${{ github.repository }}"
          ISSUE_NUMBER="${{ steps.tracking-issue.outputs.issue_number }}"
          UNFIXABLE_IDS="${{ steps.orchestrate.outputs.unfixable_alert_ids }}"
          UNFIXABLE_COUNT="${{ steps.orchestrate.outputs.unfixable_count }}"
          RUN_ID="${{ github.run_id }}"

          # Build the human review comment body
          BODY="## :warning: Alerts Requiring Human Review\n\n"
          BODY+="**${UNFIXABLE_COUNT} alert(s)** could not be automatically fixed by Devin. The files containing these alerts were not modified by the Devin session, indicating the fix was not attempted or was too complex for automated resolution.\n\n"
          BODY+="These alerts need manual investigation by a security engineer.\n\n"
          BODY+="| Alert | Link | Action Needed |\n"
          BODY+="|-------|------|---------------|\n"

          IFS=',' read -ra AIDS <<< "$UNFIXABLE_IDS"
          for aid in "${AIDS[@]}"; do
            if [ -n "$aid" ]; then
              BODY+="| #${aid} | [View alert](https://github.com/${REPO}/security/code-scanning/${aid}) | :eyes: Manual review |\n"
            fi
          done

          BODY+="\n---\n"
          BODY+="*Reported by [orchestrator run](https://github.com/${REPO}/actions/runs/${RUN_ID})*\n"
          BODY+="*These alerts were attempted by Devin but could not be resolved. Possible reasons: complex logic requiring human judgment, false positives that should be dismissed, or alerts requiring architectural changes.*"

          # Post comment on the tracking issue
          if [ -n "$ISSUE_NUMBER" ] && [ "$ISSUE_NUMBER" != "0" ]; then
            echo "Posting unfixable alerts report to issue #$ISSUE_NUMBER..."
            curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/issues/$ISSUE_NUMBER/comments" \
              -d "$(jq -n --arg body "$(echo -e "$BODY")" '{body: $body}')"
            echo "Posted unfixable alerts report."
          fi

          # Apply label for visibility
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "Authorization: token $GH_PAT" \
            "https://api.github.com/repos/$REPO/labels/devin%3Ahuman-review-needed")
          if [ "$HTTP_CODE" = "404" ]; then
            curl -s -L -X POST \
              -H "Authorization: token $GH_PAT" \
              -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/$REPO/labels" \
              -d '{"name":"devin:human-review-needed","color":"d93f0b","description":"Alerts that Devin could not auto-fix — requires human security review"}'
          fi
          curl -s -L -X POST \
            -H "Authorization: token $GH_PAT" \
            -H "Accept: application/vnd.github+json" \
            "https://api.github.com/repos/$REPO/issues/$ISSUE_NUMBER/labels" \
            -d '["devin:human-review-needed"]'
          echo "Applied devin:human-review-needed label to issue #$ISSUE_NUMBER"
